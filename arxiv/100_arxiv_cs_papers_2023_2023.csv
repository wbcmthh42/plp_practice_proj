Title,Summary,Updated,Category,Link
On the `Semantics' of Differential Privacy: A Bayesian Formulation,"  Differential privacy is a definition of ""privacy'"" for algorithms that
analyze and publish information about statistical databases. It is often
claimed that differential privacy provides guarantees against adversaries with
arbitrary side information. In this paper, we provide a precise formulation of
these guarantees in terms of the inferences drawn by a Bayesian adversary. We
show that this formulation is satisfied by both ""vanilla"" differential privacy
as well as a relaxation known as (epsilon,delta)-differential privacy. Our
formulation follows the ideas originally due to Dwork and McSherry [Dwork
2006]. This paper is, to our knowledge, the first place such a formulation
appears explicitly. The analysis of the relaxed definition is new to this
paper, and provides some concrete guidance for setting parameters when using
(epsilon,delta)-differential privacy.
",2023-01-24,Computer Science - Cryptography and Security,http://arxiv.org/abs/0803.3946
On the Dual Formulation of Boosting Algorithms,"  We study boosting algorithms from a new perspective. We show that the
Lagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with
generalized hinge loss are all entropy maximization problems. By looking at the
dual problems of these boosting algorithms, we show that the success of
boosting algorithms can be understood in terms of maintaining a better margin
distribution by maximizing margins and at the same time controlling the margin
variance.We also theoretically prove that, approximately, AdaBoost maximizes
the average margin, instead of the minimum margin. The duality formulation also
enables us to develop column generation based optimization algorithms, which
are totally corrective. We show that they exhibit almost identical
classification results to that of standard stage-wise additive boosting
algorithms but with much faster convergence rates. Therefore fewer weak
classifiers are needed to build the ensemble using our proposed optimization
technique.
",2023-05-30,Computer Science - Machine Learning,http://arxiv.org/abs/0901.3590
Domain Adaptation: Learning Bounds and Algorithms,"  This paper addresses the general problem of domain adaptation which arises in
a variety of applications where the distribution of the labeled sample
available somewhat differs from that of the test data. Building on previous
work by Ben-David et al. (2007), we introduce a novel distance between
distributions, discrepancy distance, that is tailored to adaptation problems
with arbitrary loss functions. We give Rademacher complexity bounds for
estimating the discrepancy distance from finite samples for different loss
functions. Using this distance, we derive novel generalization bounds for
domain adaptation for a wide family of loss functions. We also present a series
of novel adaptation bounds for large classes of regularization-based
algorithms, including support vector machines and kernel ridge regression based
on the empirical discrepancy. This motivates our analysis of the problem of
minimizing the empirical discrepancy for various loss functions for which we
also give novel algorithms. We report the results of preliminary experiments
that demonstrate the benefits of our discrepancy minimization algorithms for
domain adaptation.
",2023-12-04,Computer Science - Machine Learning,http://arxiv.org/abs/0902.3430
"Q-system Cluster Algebras, Paths and Total Positivity","  We review the solution of the $A_r$ Q-systems in terms of the partition
function of paths on a weighted graph, and show that it is possible to modify
the graphs and transfer matrices so as to provide an explicit connection to the
theory of planar networks introduced in the context of totally positive
matrices by Fomin and Zelevinsky.
",2023-07-12,Economics - General Economics,http://arxiv.org/abs/0906.3421
Optimization of Synthesis Oversampled Complex Filter Banks,"  An important issue with oversampled FIR analysis filter banks (FBs) is to
determine inverse synthesis FBs, when they exist. Given any complex oversampled
FIR analysis FB, we first provide an algorithm to determine whether there
exists an inverse FIR synthesis system. We also provide a method to ensure the
Hermitian symmetry property on the synthesis side, which is serviceable to
processing real-valued signals. As an invertible analysis scheme corresponds to
a redundant decomposition, there is no unique inverse FB. Given a particular
solution, we parameterize the whole family of inverses through a null space
projection. The resulting reduced parameter set simplifies design procedures,
since the perfect reconstruction constrained optimization problem is recast as
an unconstrained optimization problem. The design of optimized synthesis FBs
based on time or frequency localization criteria is then investigated, using a
simple yet efficient gradient algorithm.
",2023-01-19,Computer Science - Information Theory,http://arxiv.org/abs/0907.3654
"Message Passing for Integrating and Assessing Renewable Generation in a
  Redundant Power Grid","  A simplified model of a redundant power grid is used to study integration of
fluctuating renewable generation. The grid consists of large number of
generator and consumer nodes. The net power consumption is determined by the
difference between the gross consumption and the level of renewable generation.
The gross consumption is drawn from a narrow distribution representing the
predictability of aggregated loads, and we consider two different distributions
representing wind and solar resources. Each generator is connected to D
consumers, and redundancy is built in by connecting R of these consumers to
other generators. The lines are switchable so that at any instance each
consumer is connected to a single generator. We explore the capacity of the
renewable generation by determining the level of ""firm"" generation capacity
that can be displaced for different levels of redundancy R. We also develop
message-passing control algorithm for finding switch settings where no
generator is overloaded.
",2023-10-06,Physics - Physics and Society,http://arxiv.org/abs/0909.2358
"On the exact discretization of the classical harmonic oscillator
  equation","  We discuss the exact discretization of the classical harmonic oscillator
equation (including the inhomogeneous case and multidimensional
generalizations) with a special stress on the energy integral. We present and
suggest some numerical applications.
",2023-03-20,Mathematical Physics,http://arxiv.org/abs/0911.3672
Multi-camera Realtime 3D Tracking of Multiple Flying Animals,"  Automated tracking of animal movement allows analyses that would not
otherwise be possible by providing great quantities of data. The additional
capability of tracking in realtime - with minimal latency - opens up the
experimental possibility of manipulating sensory feedback, thus allowing
detailed explorations of the neural basis for control of behavior. Here we
describe a new system capable of tracking the position and body orientation of
animals such as flies and birds. The system operates with less than 40 msec
latency and can track multiple animals simultaneously. To achieve these
results, a multi target tracking algorithm was developed based on the Extended
Kalman Filter and the Nearest Neighbor Standard Filter data association
algorithm. In one implementation, an eleven camera system is capable of
tracking three flies simultaneously at 60 frames per second using a gigabit
network of nine standard Intel Pentium 4 and Core 2 Duo computers. This
manuscript presents the rationale and details of the algorithms employed and
shows three implementations of the system. An experiment was performed using
the tracking system to measure the effect of visual contrast on the flight
speed of Drosophila melanogaster. At low contrasts, speed is more variable and
faster on average than at high contrasts. Thus, the system is already a useful
tool to study the neurobiology and behavior of freely flying animals. If
combined with other techniques, such as `virtual reality'-type computer
graphics or genetic manipulation, the tracking system would offer a powerful
new way to investigate the biology of flying animals.
",2023-02-01,Computer Science - Computer Vision and Pattern Recognition,http://arxiv.org/abs/1001.4297
Explicit Evidence Systems with Common Knowledge,"  Justification logics are epistemic logics that explicitly include
justifications for the agents' knowledge. We develop a multi-agent
justification logic with evidence terms for individual agents as well as for
common knowledge. We define a Kripke-style semantics that is similar to
Fitting's semantics for the Logic of Proofs LP. We show the soundness,
completeness, and finite model property of our multi-agent justification logic
with respect to this Kripke-style semantics. We demonstrate that our logic is a
conservative extension of Yavorskaya's minimal bimodal explicit evidence logic,
which is a two-agent version of LP. We discuss the relationship of our logic to
the multi-agent modal logic S4 with common knowledge. Finally, we give a brief
analysis of the coordinated attack problem in the newly developed language of
our logic.
",2023-08-01,Computer Science - Logic in Computer Science,http://arxiv.org/abs/1005.0484
"Local algorithms for the maximum flow and minimum cut in bounded-degree
  networks","  We show a deterministic constant-time local algorithm for constructing an
approximately maximum flow and minimum fractional cut in
multisource-multitarget networks with bounded degrees and bounded edge
capacities. Locality means that the decision we make about each edge only
depends on its constant radius neighborhood. We show two applications of the
algorithms: one is related to the Aldous-Lyons Conjecture, and the other is
about approximating the neighborhood distribution of graphs by bounded-size
graphs. The scope of our results can be extended to unimodular random graphs
and networks. As a corollary, we generalize the Maximum Flow Minimum Cut
Theorem to unimodular random flow networks.
",2023-11-03,Computer Science - Data Structures and Algorithms,http://arxiv.org/abs/1005.0513
Nominal Unification from a Higher-Order Perspective,"  Nominal Logic is a version of first-order logic with equality, name-binding,
renaming via name-swapping and freshness of names. Contrarily to higher-order
logic, bindable names, called atoms, and instantiable variables are considered
as distinct entities. Moreover, atoms are capturable by instantiations,
breaking a fundamental principle of lambda-calculus. Despite these differences,
nominal unification can be seen from a higher-order perspective. From this
view, we show that nominal unification can be reduced to a particular fragment
of higher-order unification problems: Higher-Order Pattern Unification. This
reduction proves that nominal unification can be decided in quadratic
deterministic time, using the linear algorithm for Higher-Order Pattern
Unification. We also prove that the translation preserves most generality of
unifiers.
",2023-03-14,Computer Science - Logic in Computer Science,http://arxiv.org/abs/1005.3731
Inflection system of a language as a complex network,"  We investigate inflection structure of a synthetic language using Latin as an
example. We construct a bipartite graph in which one group of vertices
correspond to dictionary headwords and the other group to inflected forms
encountered in a given text. Each inflected form is connected to its
corresponding headword, which in some cases in non-unique. The resulting sparse
graph decomposes into a large number of connected components, to be called word
groups. We then show how the concept of the word group can be used to construct
coverage curves of selected Latin texts. We also investigate a version of the
inflection graph in which all theoretically possible inflected forms are
included. Distribution of sizes of connected components of this graphs
resembles cluster distribution in a lattice percolation near the critical
point.
",2023-12-18,Computer Science - Computation and Language,http://arxiv.org/abs/1007.1025
"Node similarity as a basic principle behind connectivity in complex
  networks","  How are people linked in a highly connected society? Since in many networks a
power-law (scale-free) node-degree distribution can be observed, power-law
might be seen as a universal characteristics of networks. But this study of
communication in the Flickr social online network reveals that power-law
node-degree distributions are restricted to only sparsely connected networks.
More densely connected networks, by contrast, show an increasing divergence
from power-law. This work shows that this observation is consistent with the
classic idea from social sciences that similarity is the driving factor behind
communication in social networks. The strong relation between communication
strength and node similarity could be confirmed by analyzing the Flickr
network. It also is shown that node similarity as a network formation model can
reproduce the characteristics of different network densities and hence can be
used as a model for describing the topological transition from weakly to
strongly connected societies.
",2023-06-22,Physics - Physics and Society,http://arxiv.org/abs/1010.0803
"Software Effort Estimation with Ridge Regression and Evolutionary
  Attribute Selection","  Software cost estimation is one of the prerequisite managerial activities
carried out at the software development initiation stages and also repeated
throughout the whole software life-cycle so that amendments to the total cost
are made. In software cost estimation typically, a selection of project
attributes is employed to produce effort estimations of the expected human
resources to deliver a software product. However, choosing the appropriate
project cost drivers in each case requires a lot of experience and knowledge on
behalf of the project manager which can only be obtained through years of
software engineering practice. A number of studies indicate that popular
methods applied in the literature for software cost estimation, such as linear
regression, are not robust enough and do not yield accurate predictions.
Recently the dual variables Ridge Regression (RR) technique has been used for
effort estimation yielding promising results. In this work we show that results
may be further improved if an AI method is used to automatically select
appropriate project cost drivers (inputs) for the technique. We propose a
hybrid approach combining RR with a Genetic Algorithm, the latter evolving the
subset of attributes for approximating effort more accurately. The proposed
hybrid cost model has been applied on a widely known high-dimensional dataset
of software project samples and the results obtained show that accuracy may be
increased if redundant attributes are eliminated.
",2023-12-21,Computer Science - Software Engineering,http://arxiv.org/abs/1012.5754
"On the effect of the path length and transitivity of small-world
  networks on epidemic dynamics","  We show how one can trace in a systematic way the coarse-grained solutions of
individual-based stochastic epidemic models evolving on heterogeneous complex
networks with respect to their topological characteristics. In particular, we
have developed algorithms that allow the tuning of the transitivity (clustering
coefficient) and the average mean-path length allowing the investigation of the
""pure"" impacts of the two characteristics on the emergent behavior of detailed
epidemic models. The framework could be used to shed more light into the
influence of weak and strong social ties on epidemic spread within small-world
network structures, and ultimately to provide novel systematic computational
modeling and exploration of better contagion control strategies.
",2023-03-24,Computer Science - Social and Information Networks,http://arxiv.org/abs/1102.5420
Well structured program equivalence is highly undecidable,"  We show that strict deterministic propositional dynamic logic with
intersection is highly undecidable, solving a problem in the Stanford
Encyclopedia of Philosophy. In fact we show something quite a bit stronger. We
introduce the construction of program equivalence, which returns the value
$\mathsf{T}$ precisely when two given programs are equivalent on halting
computations. We show that virtually any variant of propositional dynamic logic
has $\Pi_1^1$-hard validity problem if it can express even just the equivalence
of well-structured programs with the empty program \texttt{skip}. We also show,
in these cases, that the set of propositional statements valid over finite
models is not recursively enumerable, so there is not even an axiomatisation
for finitely valid propositions.
",2023-11-08,Computer Science - Logic in Computer Science,http://arxiv.org/abs/1103.1433
"A Discrete Adapted Hierarchical Basis Solver For Radial Basis Function
  Interpolation","  In this paper we develop a discrete Hierarchical Basis (HB) to efficiently
solve the Radial Basis Function (RBF) interpolation problem with variable
polynomial order. The HB forms an orthogonal set and is adapted to the kernel
seed function and the placement of the interpolation nodes. Moreover, this
basis is orthogonal to a set of polynomials up to a given order defined on the
interpolating nodes. We are thus able to decouple the RBF interpolation problem
for any order of the polynomial interpolation and solve it in two steps: (1)
The polynomial orthogonal RBF interpolation problem is efficiently solved in
the transformed HB basis with a GMRES iteration and a diagonal, or block SSOR
preconditioner. (2) The residual is then projected onto an orthonormal
polynomial basis. We apply our approach on several test cases to study its
effectiveness, including an application to the Best Linear Unbiased Estimator
regression problem.
",2023-11-21,Mathematics - Numerical Analysis,http://arxiv.org/abs/1104.2504
Towards joint decoding of binary Tardos fingerprinting codes,"  The class of joint decoder of probabilistic fingerprinting codes is of utmost
importance in theoretical papers to establish the concept of fingerprint
capacity. However, no implementation supporting a large user base is known to
date. This article presents an iterative decoder which is, as far as we are
aware of, the first practical attempt towards joint decoding. The
discriminative feature of the scores benefits on one hand from the
side-information of previously accused users, and on the other hand, from
recently introduced universal linear decoders for compound channels. Neither
the code construction nor the decoder make precise assumptions about the
collusion (size or strategy). The extension to incorporate soft outputs from
the watermarking layer is straightforward. An extensive experimental work
benchmarks the very good performance and offers a clear comparison with
previous state-of-the-art decoders.
",2023-07-26,Computer Science - Information Theory,http://arxiv.org/abs/1104.5616
Characteristics of Optimal Solutions to the Sensor Location Problem,"  In [Bianco, L., Giuseppe C., and P. Reverberi. 2001. ""A network based model
for traffic sensor location with implications on O/D matrix estimates"".
Transportation Science 35(1):50-60.], the authors present the Sensor Location
Problem: that of locating the minimum number of traffic sensors at
intersections of a road network such that the traffic flow on the entire
network can be determined. They offer a necessary and sufficient condition on
the set of monitored nodes in order for the flow everywhere to be determined.
In this paper, we present a counterexample that demonstrates that the condition
is not actually sufficient (though it is still necessary). We present a
stronger necessary condition for flow calculability, and show that it is a
sufficient condition in a large class of graphs in which a particular subgraph
is a tree. Many typical road networks are included in this category, and we
show how our condition can be used to inform traffic sensor placement.
",2023-09-28,Mathematics - Optimization and Control,http://arxiv.org/abs/1106.0349
Model of communities isolation at hierarchical modular networks,"  The model of community isolation was extended to the case when individuals
are randomly placed at nodes of hierarchical modular networks. It was shown
that the average number of blocked nodes (individuals) increases in time as a
power function, with the exponent depending on network parameters. The
distribution of time when the first isolated cluster appears is unimodal,
non-gaussian. The developed analytical approach is in a good agreement with the
simulation data.
",2023-07-19,Physics - Physics and Society,http://arxiv.org/abs/1106.0439
"Another approach to the equivalence of measure-many one-way quantum
  finite automata and its application","  In this paper, we present a much simpler, direct and elegant approach to the
equivalence problem of {\it measure many one-way quantum finite automata}
(MM-1QFAs). The approach is essentially generalized from the work of Carlyle
[J. Math. Anal. Appl. 7 (1963) 167-175]. Namely, we reduce the equivalence
problem of MM-1QFAs to that of two (initial) vectors.
  As an application of the approach, we utilize it to address the equivalence
problem of {\it Enhanced one-way quantum finite automata} (E-1QFAs) introduced
by Nayak [Proceedings of the 40th Annual IEEE Symposium on Foundations of
Computer Science, 1999, pp.~369-376]. We prove that two E-1QFAs $\mathcal{A}_1$
and $\mathcal{A}_2$ over $\Sigma$ are equivalence if and only if they are
$n_1^2+n_2^2-1$-equivalent where $n_1$ and $n_2$ are the numbers of states in
$\mathcal{A}_1$ and $\mathcal{A}_2$, respectively.
",2023-06-19,Computer Science - Computational Complexity,http://arxiv.org/abs/1106.2481
Some results on equivalence of multi-letter quantum finite automata,"  Two quantum finite automata are equivalent if for all input string $\omega$
over the input alphabet the two automata accept $\omega$ with equal
probability. In [Theoret. Comput. Sci. 410 (2009) 3006-3017], it was shown that
a $k_1$-letter QFA $\mathcal{A}_1$ and a $k_2$-letter QFA $\mathcal{A}_2$ over
$\Sigma=\{\sigma\}$, are equivalent if and only if they are
$(n_1+n_2)^4+k-1$-equivalent where $n_i$ is the number of states of
$\mathcal{A}_i$, $i=1,2$, and $k=\max\{k_1,k_2\}$. In this letter, we improve
the above upper-bound to $(n_1^2+n_2^2-1)+k$. This also answers an open problem
of Qiu et al. [Acta Informatica 48 (2011) 271-290]. Further, we show that, in
the case of $\Sigma=\{\sigma_1,...,\sigma_t\}$ with $2\leq t<\infty$, there
exists an integer $z$ such that $\mathcal{A}_1$ and $\mathcal{A}_2$ are
equivalent if and only if they satisfy $z$-equivalent.
",2023-06-06,Computer Science - Computational Complexity,http://arxiv.org/abs/1106.5223
Edit wars in Wikipedia,"  We present a new, efficient method for automatically detecting severe
conflicts `edit wars' in Wikipedia and evaluate this method on six different
language WPs. We discuss how the number of edits, reverts, the length of
discussions, the burstiness of edits and reverts deviate in such pages from
those following the general workflow, and argue that earlier work has
significantly over-estimated the contentiousness of the Wikipedia editing
process.
",2023-01-05,Statistics - Machine Learning,http://arxiv.org/abs/1107.3689
Dynamics in Near-Potential Games,"  Except for special classes of games, there is no systematic framework for
analyzing the dynamical properties of multi-agent strategic interactions.
Potential games are one such special but restrictive class of games that allow
for tractable dynamic analysis. Intuitively, games that are ""close"" to a
potential game should share similar properties. In this paper, we formalize and
develop this idea by quantifying to what extent the dynamic features of
potential games extend to ""near-potential"" games. We study convergence of three
commonly studied classes of adaptive dynamics: discrete-time better/best
response, logit response, and discrete-time fictitious play dynamics. For
better/best response dynamics, we focus on the evolution of the sequence of
pure strategy profiles and show that this sequence converges to a (pure)
approximate equilibrium set, whose size is a function of the ""distance"" from a
close potential game. We then study logit response dynamics and provide a
characterization of the stationary distribution of this update rule in terms of
the distance of the game from a close potential game and the corresponding
potential function. We further show that the stochastically stable strategy
profiles are pure approximate equilibria. Finally, we turn attention to
fictitious play, and establish that the sequence of empirical frequencies of
player actions converges to a neighborhood of (mixed) equilibria of the game,
where the size of the neighborhood increases with distance of the game to a
potential game. Thus, our results suggest that games that are close to a
potential game inherit the dynamical properties of potential games. Since a
close potential game to a given game can be found by solving a convex
optimization problem, our approach also provides a systematic framework for
studying convergence behavior of adaptive learning dynamics in arbitrary finite
strategic form games.
",2023-10-03,Computer Science - Computer Science and Game Theory,http://arxiv.org/abs/1107.4386
"Simple characterizations for commutativity of quantum weakest
  preconditions","  In a recent letter [Information Processing Letters~104 (2007) 152-158], it
has shown some sufficient conditions for commutativity of quantum weakest
preconditions. This paper provides some alternative and simple
characterizations for the commutativity of quantum weakest preconditions, i.e.,
Theorem 3.1, Theorem 3.2 and Proposition 3.3 in what follows. We also show that
to characterize the commutativity of quantum weakest preconditions in terms of
$[M,N]$ ($=MN-NM$) is hard in the sense of Proposition 4.1 and Proposition 4.2.
",2023-07-19,Computer Science - Logic in Computer Science,http://arxiv.org/abs/1108.4607
Emotional Analysis of Blogs and Forums Data,"  We perform a statistical analysis of emotionally annotated comments in two
large online datasets, examining chains of consecutive posts in the
discussions. Using comparisons with randomised data we show that there is a
high level of correlation for the emotional content of messages.
",2023-07-19,Computer Science - Computation and Language,http://arxiv.org/abs/1108.5974
"Kripke Semantics for Martin-L\""of's Extensional Type Theory","  It is well-known that simple type theory is complete with respect to
non-standard set-valued models. Completeness for standard models only holds
with respect to certain extended classes of models, e.g., the class of
cartesian closed categories. Similarly, dependent type theory is complete for
locally cartesian closed categories. However, it is usually difficult to
establish the coherence of interpretations of dependent type theory, i.e., to
show that the interpretations of equal expressions are indeed equal. Several
classes of models have been used to remedy this problem. We contribute to this
investigation by giving a semantics that is standard, coherent, and
sufficiently general for completeness while remaining relatively easy to
compute with. Our models interpret types of Martin-L\""of's extensional
dependent type theory as sets indexed over posets or, equivalently, as
fibrations over posets. This semantics can be seen as a generalization to
dependent type theory of the interpretation of intuitionistic first-order logic
in Kripke models. This yields a simple coherent model theory, with respect to
which simple and dependent type theory are sound and complete.
",2023-03-31,Computer Science - Logic in Computer Science,http://arxiv.org/abs/1109.1702
"Circadian patterns of Wikipedia editorial activity: A demographic
  analysis","  Wikipedia (WP) as a collaborative, dynamical system of humans is an
appropriate subject of social studies. Each single action of the members of
this society, i.e. editors, is well recorded and accessible. Using the
cumulative data of 34 Wikipedias in different languages, we try to characterize
and find the universalities and differences in temporal activity patterns of
editors. Based on this data, we estimate the geographical distribution of
editors for each WP in the globe. Furthermore we also clarify the differences
among different groups of WPs, which originate in the variance of cultural and
social features of the communities of editors.
",2023-01-05,Physics - Physics and Society,http://arxiv.org/abs/1109.1746
Relative Error Control in Bivariate Interpolatory Cubature,"  We describe an algorithm for controlling the relative error in the numerical
evaluation of a bivariate integral, without prior knowledge of the magnitude of
the integral. In the event that the magnitude of the integral is less than
unity, absolute error control is preferred. The underlying quadrature rule is
positive-weight interpolatory and composite. Some numerical examples
demonstrate the algorithm.
",2023-12-12,Mathematics - Numerical Analysis,http://arxiv.org/abs/1110.2324
Nystrom Methods in the RKQ Algorithm for Initial-value Problems,"  We incorporate explicit Nystrom methods into the RKQ algorithm for stepwise
global error control in numerical solutions of initial-value problems. The
initial-value problem is transformed into an explicitly second-order problem,
so as to be suitable for Nystrom integration. The Nystrom methods used are
fourth-order, fifth-order and 10th-order. Two examples demonstrate the
effectiveness of the algorithm.
",2023-11-28,Mathematics - Numerical Analysis,http://arxiv.org/abs/1110.6749
Data Processing For Atomic Resolution EELS,"  The high beam current and sub-angstrom resolution of aberration-corrected
scanning transmission electron microscopes has enabled electron energy loss
spectroscopic (EELS) mapping with atomic resolution. These spectral maps are
often dose-limited and spatially oversampled, leading to low counts/channel and
are thus highly sensitive to errors in background estimation. However, by
taking advantage of redundancy in the dataset map one can improve background
estimation and increase chemical sensitivity. We consider two such approaches-
linear combination of power laws and local background averaging-that reduce
background error and improve signal extraction. Principal components analysis
(PCA) can also be used to analyze spectrum images, but the poor
peak-to-background ratio in EELS can lead to serious artifacts if raw EELS data
is PCA filtered. We identify common artifacts and discuss alternative
approaches. These algorithms are implemented within the Cornell Spectrum
Imager, an open source software package for spectroscopic analysis.
",2023-07-19,Condensed Matter - Materials Science,http://arxiv.org/abs/1112.3059
A Novel Chaotic Image Encryption using Generalized Threshold Function,"  In this paper, after reviewing the main points of image encryption and
threshold function, we introduce the methods of chaotic image encryption based
on pseudorandom bit padding that the bits be generated by the novel generalized
threshold function (segmentation and self-similarity) methods. These methods
decrease periodic effect of the ergodic dynamical systems in randomness of the
chaotic image encryption. The essential idea of this paper is that given
threshold functions of the ergodic dynamical systems. To evaluate the security
of the cipher image of this scheme, the key space analysis, the correlation of
two adjacent pixels and differential attack were performed. This scheme tries
to improve the problem of failure of encryption such as small key space and
level of security.
",2023-07-19,Computer Science - Cryptography and Security,http://arxiv.org/abs/1112.3791
Inductive types in homotopy type theory,"  Homotopy type theory is an interpretation of Martin-L\""of's constructive type
theory into abstract homotopy theory. There results a link between constructive
mathematics and algebraic topology, providing topological semantics for
intensional systems of type theory as well as a computational approach to
algebraic topology via type theory-based proof assistants such as Coq.
  The present work investigates inductive types in this setting. Modified rules
for inductive types, including types of well-founded trees, or W-types, are
presented, and the basic homotopical semantics of such types are determined.
Proofs of all results have been formally verified by the Coq proof assistant,
and the proof scripts for this verification form an essential component of this
research.
",2023-03-31,Mathematics - Logic,http://arxiv.org/abs/1201.3898
"Orthogonal Multiple Access with Correlated Sources: Feasible Region and
  Pragmatic Schemes","  In this paper, we consider orthogonal multiple access coding schemes, where
correlated sources are encoded in a distributed fashion and transmitted,
through additive white Gaussian noise (AWGN) channels, to an access point (AP).
At the AP, component decoders, associated with the source encoders, iteratively
exchange soft information by taking into account the source correlation. The
first goal of this paper is to investigate the ultimate achievable performance
limits in terms of a multi-dimensional feasible region in the space of channel
parameters, deriving insights on the impact of the number of sources. The
second goal is the design of pragmatic schemes, where the sources use
""off-the-shelf"" channel codes. In order to analyze the performance of given
coding schemes, we propose an extrinsic information transfer (EXIT)-based
approach, which allows to determine the corresponding multi-dimensional
feasible regions. On the basis of the proposed analytical framework, the
performance of pragmatic coded schemes, based on serially concatenated
convolutional codes (SCCCs), is discussed.
",2023-02-08,Computer Science - Information Theory,http://arxiv.org/abs/1201.6548
"rFerns: An Implementation of the Random Ferns Method for General-Purpose
  Machine Learning","  In this paper I present an extended implementation of the Random ferns
algorithm contained in the R package rFerns. It differs from the original by
the ability of consuming categorical and numerical attributes instead of only
binary ones. Also, instead of using simple attribute subspace ensemble it
employs bagging and thus produce error approximation and variable importance
measure modelled after Random forest algorithm. I also present benchmarks'
results which show that although Random ferns' accuracy is mostly smaller than
achieved by Random forest, its speed and good quality of importance measure it
provides make rFerns a reasonable choice for a specific applications.
",2023-08-15,Computer Science - Machine Learning,http://arxiv.org/abs/1202.1121
Variants of local algorithms on sparse graphs,"  Suppose we want to construct some structure on a bounded-degree graph, e.g.,
an almost maximum matching, and we want to decide about each edge depending
only on its constant-radius neighborhood. We examine and compare the strengths
of different extensions of these local algorithms. A common extension is to use
preprocessing, which means that we can make some calculation about the whole
graph, and each local decision can also depend on this calculation. In this
paper, we show that preprocessing is needless: if a nearly optimal local
algorithm uses preprocessing, then the same can be achieved by a local
algorithm without preprocessing, but with a global randomization.
",2023-11-03,Mathematics - Combinatorics,http://arxiv.org/abs/1202.1565
Dynamics of conflicts in Wikipedia,"  In this work we study the dynamical features of editorial wars in Wikipedia
(WP). Based on our previously established algorithm, we build up samples of
controversial and peaceful articles and analyze the temporal characteristics of
the activity in these samples. On short time scales, we show that there is a
clear correspondence between conflict and burstiness of activity patterns, and
that memory effects play an important role in controversies. On long time
scales, we identify three distinct developmental patterns for the overall
behavior of the articles. We are able to distinguish cases eventually leading
to consensus from those cases where a compromise is far from achievable.
Finally, we analyze discussion networks and conclude that edit wars are mainly
fought by few editors only.
",2023-01-05,Physics - Physics and Society,http://arxiv.org/abs/1202.3643
"On equivalence, languages equivalence and minimization of multi-letter
  and multi-letter measure-many quantum automata","  We first show that given a $k_1$-letter quantum finite automata
$\mathcal{A}_1$ and a $k_2$-letter quantum finite automata $\mathcal{A}_2$ over
the same input alphabet $\Sigma$, they are equivalent if and only if they are
$(n_1^2+n_2^2-1)|\Sigma|^{k-1}+k$-equivalent where $n_1$, $i=1,2$, are the
numbers of state in $\mathcal{A}_i$ respectively, and $k=\max\{k_1,k_2\}$. By
applying a method, due to the author, used to deal with the equivalence problem
of {\it measure many one-way quantum finite automata}, we also show that a
$k_1$-letter measure many quantum finite automaton $\mathcal{A}_1$ and a
$k_2$-letter measure many quantum finite automaton $\mathcal{A}_2$ are
equivalent if and only if they are $(n_1^2+n_2^2-1)|\Sigma|^{k-1}+k$-equivalent
where $n_i$, $i=1,2$, are the numbers of state in $\mathcal{A}_i$ respectively,
and $k=\max\{k_1,k_2\}$.
  Next, we study the language equivalence problem of those two kinds of quantum
finite automata. We show that for $k$-letter quantum finite automata, the
non-strict cut-point language equivalence problem is undecidable, i.e., it is
undecidable whether
$L_{\geq\lambda}(\mathcal{A}_1)=L_{\geq\lambda}(\mathcal{A}_2)$ where
$0<\lambda\leq 1$ and $\mathcal{A}_i$ are $k_i$-letter quantum finite automata.
Further, we show that both strict and non-strict cut-point language equivalence
problem for $k$-letter measure many quantum finite automata are undecidable.
The direct consequences of the above outcomes are summarized in the paper.
  Finally, we comment on existing proofs about the minimization problem of one
way quantum finite automata not only because we have been showing great
interest in this kind of problem, which is very important in classical automata
theory, but also due to that the problem itself, personally, is a challenge.
This problem actually remains open.
",2023-07-21,Computer Science - Computational Complexity,http://arxiv.org/abs/1203.0113
"Posterior Mean Super-Resolution with a Compound Gaussian Markov Random
  Field Prior","  This manuscript proposes a posterior mean (PM) super-resolution (SR) method
with a compound Gaussian Markov random field (MRF) prior. SR is a technique to
estimate a spatially high-resolution image from observed multiple
low-resolution images. A compound Gaussian MRF model provides a preferable
prior for natural images that preserves edges. PM is the optimal estimator for
the objective function of peak signal-to-noise ratio (PSNR). This estimator is
numerically determined by using variational Bayes (VB). We then solve the
conjugate prior problem on VB and the exponential-order calculation cost
problem of a compound Gaussian MRF prior with simple Taylor approximations. In
experiments, the proposed method roughly overcomes existing methods.
",2023-06-01,Computer Science - Computer Vision and Pattern Recognition,http://arxiv.org/abs/1203.0781
A practical approach to language complexity: a Wikipedia case study,"  In this paper we present statistical analysis of English texts from
Wikipedia. We try to address the issue of language complexity empirically by
comparing the simple English Wikipedia (Simple) to comparable samples of the
main English Wikipedia (Main). Simple is supposed to use a more simplified
language with a limited vocabulary, and editors are explicitly requested to
follow this guideline, yet in practice the vocabulary richness of both samples
are at the same level. Detailed analysis of longer units (n-grams of words and
part of speech tags) shows that the language of Simple is less complex than
that of Main primarily due to the use of shorter sentences, as opposed to
drastically simplified syntax or vocabulary. Comparing the two language
varieties by the Gunning readability index supports this conclusion. We also
report on the topical dependence of language complexity, e.g. that the language
is more advanced in conceptual articles compared to person-based (biographical)
and object-based articles. Finally, we investigate the relation between
conflict and language complexity by analyzing the content of the talk pages
associated to controversial and peacefully developing articles, concluding that
controversy has the effect of reducing language complexity.
",2023-01-05,Computer Science - Computation and Language,http://arxiv.org/abs/1204.2765
Quantitative Concept Analysis,"  Formal Concept Analysis (FCA) begins from a context, given as a binary
relation between some objects and some attributes, and derives a lattice of
concepts, where each concept is given as a set of objects and a set of
attributes, such that the first set consists of all objects that satisfy all
attributes in the second, and vice versa. Many applications, though, provide
contexts with quantitative information, telling not just whether an object
satisfies an attribute, but also quantifying this satisfaction. Contexts in
this form arise as rating matrices in recommender systems, as occurrence
matrices in text analysis, as pixel intensity matrices in digital image
processing, etc. Such applications have attracted a lot of attention, and
several numeric extensions of FCA have been proposed. We propose the framework
of proximity sets (proxets), which subsume partially ordered sets (posets) as
well as metric spaces. One feature of this approach is that it extracts from
quantified contexts quantified concepts, and thus allows full use of the
available information. Another feature is that the categorical approach allows
analyzing any universal properties that the classical FCA and the new versions
may have, and thus provides structural guidance for aligning and combining the
approaches.
",2023-11-03,Computer Science - Machine Learning,http://arxiv.org/abs/1204.5802
Combinatorial aspect of fashion,"  Simulations are performed according to the Axelrod model of culture
dissemination, with modified mechanism of repulsion. Previously, repulsion was
considered by Radillo-Diaz et al (Phys. Rev. E 80 (2009) 066107) as dependent
on a predefined threshold. Here the probabilities of attraction and repulsion
are calculated from the number of cells in the same states. We also investigate
the influence of some homogeneity, introduced to the initial state. As the
result of the probabilistic definition of repulsion, the ordered state
vanishes. A small cluster of a few percent of population is retained only if in
the initial state a set of agents is prepared in the same state. We conclude
that the modelled imitation is successful only with respect to agents, and not
only their features.
",2023-07-19,Physics - Physics and Society,http://arxiv.org/abs/1205.2251
"The Longest Queue Drop Policy for Shared-Memory Switches is
  1.5-competitive","  We consider the Longest Queue Drop memory management policy in shared-memory
switches consisting of $N$ output ports. The shared memory of size $M\geq N$
may have an arbitrary number of input ports. Each packet may be admitted by any
incoming port, but must be destined to a specific output port and each output
port may be used by only one queue. The Longest Queue Drop policy is a natural
online strategy used in directing the packet flow in buffering problems.
According to this policy and assuming unit packet values and cost of
transmission, every incoming packet is accepted, whereas if the shared memory
becomes full, one or more packets belonging to the longest queue are preempted,
in order to make space for the newly arrived packets. It was proved in 2001
[Hahne et al., SPAA '01] that the Longest Queue Drop policy is 2-competitive
and at least $\sqrt{2}$-competitive. It remained an open question whether a
(2-\epsilon) upper bound for the competitive ratio of this policy could be
shown, for any positive constant \epsilon. We show that the Longest Queue Drop
online policy is 1.5-competitive.
",2023-02-09,Computer Science - Data Structures and Algorithms,http://arxiv.org/abs/1207.1141
"Opinions, Conflicts and Consensus: Modeling Social Dynamics in a
  Collaborative Environment","  Information-communication technology promotes collaborative environments like
Wikipedia where, however, controversiality and conflicts can appear. To
describe the rise, persistence, and resolution of such conflicts we devise an
extended opinion dynamics model where agents with different opinions perform a
single task to make a consensual product. As a function of the convergence
parameter describing the influence of the product on the agents, the model
shows spontaneous symmetry breaking of the final consensus opinion represented
by the medium. In the case when agents are replaced with new ones at a certain
rate, a transition from mainly consensus to a perpetual conflict occurs, which
is in qualitative agreement with the scenarios observed in Wikipedia.
",2023-01-05,Physics - Physics and Society,http://arxiv.org/abs/1207.4914
A Session based Multiple Image Hiding Technique using DWT and DCT,"  This work proposes Steganographic technique for hiding multiple images in a
color image based on DWT and DCT. The cover image is decomposed into three
separate color planes namely R, G and B. Individual planes are decomposed into
subbands using DWT. DCT is applied in HH component of each plane. Secret images
are dispersed among the selected DCT coefficients using a pseudo random
sequence and a Session key. Secret images are extracted using the session key
and the size of the images from the planer decomposed stego image. In this
approach the stego image generated is of acceptable level of imperceptibility
and distortion compared to the cover image and the overall security is high.
",2023-07-19,Computer Science - Cryptography and Security,http://arxiv.org/abs/1208.0950
Value production in a collaborative environment,"  We review some recent endeavors and add some new results to characterize and
understand underlying mechanisms in Wikipedia (WP), the paradigmatic example of
collaborative value production. We analyzed the statistics of editorial
activity in different languages and observed typical circadian and weekly
patterns, which enabled us to estimate the geographical origins of
contributions to WPs in languages spoken in several time zones. Using a
recently introduced measure we showed that the editorial activities have
intrinsic dependencies in the burstiness of events. A comparison of the English
and Simple English WPs revealed important aspects of language complexity and
showed how peer cooperation solved the task of enhancing readability. One of
our focus issues was characterizing the conflicts or edit wars in WPs, which
helped us to automatically filter out controversial pages. When studying the
temporal evolution of the controversiality of such pages we identified typical
patterns and classified conflicts accordingly. Our quantitative analysis
provides the basis of modeling conflicts and their resolution in collaborative
environments and contribute to the understanding of this issue, which becomes
increasingly important with the development of information communication
technology.
",2023-01-05,Physics - Physics and Society,http://arxiv.org/abs/1208.5130
Proof of Church's Thesis,"  We prove that if our calculating capability is that of a universal Turing
machine with a finite tape, then Church's thesis is true. This way we
accomplish Post (1936) program.
",2023-03-22,Computer Science - Logic in Computer Science,http://arxiv.org/abs/1209.5036
"Network structure of phonographic market with characteristic
  similarities between musicians","  We investigate relations between best selling artists in last decade on
phonographic market and from perspective of listeners by using the Social
Network Analyzes. Starting network is obtained from the matrix of correlations
between the world's best selling artists by considering the synchronous time
evolution of weekly record sales. This method reveals the structure of
phonographic market, but we claim that it has no impact on people who see
relationship between artists and music genres. We compare 'sale' (based on
correlation of record sales) or 'popularity' (based on data mining of the
record charts) networks with 'similarity' (obtained mainly from survey within
music experts opinion) and find no significant relations. We postulate that
non-laminar phenomena on this specific market introduce turbulence to how
people view relations of artists.
",2023-07-19,Nonlinear Sciences - Adaptation and Self-Organizing Systems,http://arxiv.org/abs/1210.0225
Enumerating topological $(n_k)$-configurations,"  An $(n_k)$-configuration is a set of $n$ points and $n$ lines in the
projective plane such that their point-line incidence graph is $k$-regular. The
configuration is geometric, topological, or combinatorial depending on whether
lines are considered to be straight lines, pseudolines, or just combinatorial
lines. We provide an algorithm for generating, for given $n$ and $k$, all
topological $(n_k)$-configurations up to combinatorial isomorphism, without
enumerating first all combinatorial $(n_k)$-configurations. We apply this
algorithm to confirm efficiently a former result on topological
$(18_4)$-configurations, from which we obtain a new geometric
$(18_4)$-configuration. Preliminary results on $(19_4)$-configurations are also
briefly reported.
",2023-11-14,Computer Science - Computational Geometry,http://arxiv.org/abs/1210.0306
Feature Subset Selection for Software Cost Modelling and Estimation,"  Feature selection has been recently used in the area of software engineering
for improving the accuracy and robustness of software cost models. The idea
behind selecting the most informative subset of features from a pool of
available cost drivers stems from the hypothesis that reducing the
dimensionality of datasets will significantly minimise the complexity and time
required to reach to an estimation using a particular modelling technique. This
work investigates the appropriateness of attributes, obtained from empirical
project databases and aims to reduce the cost drivers used while preserving
performance. Finding suitable subset selections that may cater improved
predictions may be considered as a pre-processing step of a particular
technique employed for cost estimation (filter or wrapper) or an internal
(embedded) step to minimise the fitting error. This paper compares nine
relatively popular feature selection methods and uses the empirical values of
selected attributes recorded in the ISBSG and Desharnais datasets to estimate
software development effort.
",2023-12-21,Computer Science - Software Engineering,http://arxiv.org/abs/1210.1161
A New Quantum Data Processing Inequality,"  Quantum data processing inequality bounds the set of bipartite states that
can be generated by two far apart parties under local operations; Having access
to a bipartite state as a resource, two parties cannot locally transform it to
another bipartite state with a mutual information greater than that of the
resource state. But due to the additivity of quantum mutual information under
tensor product, the data processing inequality gives no bound when the parties
are provided with arbitrary number of copies of the resource state. In this
paper we introduce a measure of correlation on bipartite quantum states, called
maximal correlation, that is not additive and gives the same number when
computed for multiple copies. Then by proving a data processing inequality for
this measure, we find a bound on the set of states that can be generated under
local operations even when an arbitrary number of copies of the resource state
is available.
",2023-03-14,Quantum Physics,http://arxiv.org/abs/1210.1689
"Vortices within vortices: hierarchical nature of vortex tubes in
  turbulence","  The JHU turbulence database [1] can be used with a state of the art
visualisation tool [2] to generate high quality fluid dynamics videos. In this
work we investigate the classical idea that smaller structures in turbulent
flows, while engaged in their own internal dynamics, are advected by the larger
structures. They are not advected undistorted, however. We see instead that the
small scale structures are sheared and twisted by the larger scales. This
illuminates the basic mechanisms of the turbulent cascade.
",2023-11-22,Physics - Fluid Dynamics,http://arxiv.org/abs/1210.3325
"Get my pizza right: Repairing missing is-a relations in ALC ontologies
  (extended version)","  With the increased use of ontologies in semantically-enabled applications,
the issue of debugging defects in ontologies has become increasingly important.
These defects can lead to wrong or incomplete results for the applications.
Debugging consists of the phases of detection and repairing. In this paper we
focus on the repairing phase of a particular kind of defects, i.e. the missing
relations in the is-a hierarchy. Previous work has dealt with the case of
taxonomies. In this work we extend the scope to deal with ALC ontologies that
can be represented using acyclic terminologies. We present algorithms and
discuss a system.
",2023-11-13,Computer Science - Artificial Intelligence,http://arxiv.org/abs/1210.7154
"Early Prediction of Movie Box Office Success based on Wikipedia Activity
  Big Data","  Use of socially generated ""big data"" to access information about collective
states of the minds in human societies has become a new paradigm in the
emerging field of computational social science. A natural application of this
would be the prediction of the society's reaction to a new product in the sense
of popularity and adoption rate. However, bridging the gap between ""real time
monitoring"" and ""early predicting"" remains a big challenge. Here we report on
an endeavor to build a minimalistic predictive model for the financial success
of movies based on collective activity data of online users. We show that the
popularity of a movie can be predicted much before its release by measuring and
analyzing the activity level of editors and viewers of the corresponding entry
to the movie in Wikipedia, the well-known online encyclopedia.
",2023-01-05,Physics - Physics and Society,http://arxiv.org/abs/1211.0970
A Linear Kernel for Planar Total Dominating Set,"  A total dominating set of a graph $G=(V,E)$ is a subset $D \subseteq V$ such
that every vertex in $V$ is adjacent to some vertex in $D$. Finding a total
dominating set of minimum size is NP-hard on planar graphs and W[2]-complete on
general graphs when parameterized by the solution size. By the meta-theorem of
Bodlaender et al. [J. ACM, 2016], there exists a linear kernel for Total
Dominating Set on graphs of bounded genus. Nevertheless, it is not clear how
such a kernel can be effectively constructed, and how to obtain explicit
reduction rules with reasonably small constants. Following the approach of
Alber et al. [J. ACM, 2004], we provide an explicit kernel for Total Dominating
Set on planar graphs with at most $410k$ vertices, where $k$ is the size of the
solution. This result complements several known constructive linear kernels on
planar graphs for other domination problems such as Dominating Set, Edge
Dominating Set, Efficient Dominating Set, Connected Dominating Set, or Red-Blue
Dominating Set.
",2023-06-22,Computer Science - Data Structures and Algorithms,http://arxiv.org/abs/1211.0978
"Systematic Stochastic Reduction of Inertial Fluid-Structure Interactions
  subject to Thermal Fluctuations","  We investigate the dynamics of elastic microstructures within a fluid that
are subjected to thermal fluctuations. We perform analysis to obtain
systematically simplified descriptions of the mechanics in the limiting regimes
when (i) the coupling forces that transfer momentum between the fluid and
microstructures is strong, (ii) the mass of the microstructures is small
relative to the displaced mass of the fluid, and (iii) the response to stresses
results in hydrodynamics that relax rapidly to a quasi-steady-state relative to
the motions of the microstructure. We derive effective equations using a
singular perturbation analysis of the Backward Kolmogorov equations of the
stochastic process. Our continuum mechanics description is based on the
Stochastic Eulerian Lagrangian Method (SELM) which provides a framework for
approximation of the fluid-structure interactions when subject to thermal
fluctuations.
",2023-10-24,Condensed Matter - Soft Condensed Matter,http://arxiv.org/abs/1211.3798
Choice Disjunctive Queries in Logic Programming,"  One of the long-standing research problems on logic programming is to treat
the cut predicate in a logical, high-level way. We argue that this problem can
be solved by adopting linear logic and choice-disjunctive goal formulas of the
form $G_0 \add G_1$ where $G_0, G_1$ are goals. These goals have the following
intended semantics: $choose$ the true disjunct $G_i$ and execute $G_i$ where $i
(= 0\ {\rm or}\ 1)$, while $discarding$ the unchosen disjunct. Note that only
one goal can remain alive during execution. These goals thus allow us to
specify mutually exclusive tasks in a high-level way.
",2023-01-31,Computer Science - Logic in Computer Science,http://arxiv.org/abs/1211.6940
Modal Functional (Dialectica) Interpretation,"  We adapt our light Dialectica interpretation to usual and light modal
formulas (with universal quantification on boolean and natural variables) and
prove it sound for a non-standard modal arithmetic based on Goedel's T and
classical S4. The range of this light modal Dialectica is the usual (non-modal)
classical Arithmetic in all finite types (with booleans); the propositional
kernel of its domain is Boolean and not S4. The `heavy' modal Dialectica
interpretation is a new technique, as it cannot be simulated within our
previous light Dialectica. The synthesized functionals are at least as good as
before, while the translation process is improved. Through our modal
Dialectica, the existence of a realizer for the defining axiom of classical S5
reduces to the Drinking Principle (cf. Smullyan).
",2023-06-22,Computer Science - Logic in Computer Science,http://arxiv.org/abs/1212.0020
"The projector algorithm: a simple parallel algorithm for computing
  Voronoi diagrams and Delaunay graphs","  The Voronoi diagram is a certain geometric data structure which has numerous
applications in various scientific and technological fields. The theory of
algorithms for computing 2D Euclidean Voronoi diagrams of point sites is rich
and useful, with several different and important algorithms. However, this
theory has been quite steady during the last few decades in the sense that no
essentially new algorithms have entered the game. In addition, most of the
known algorithms are serial in nature and hence cast inherent difficulties on
the possibility to compute the diagram in parallel. In this paper we present
the projector algorithm: a new and simple algorithm which enables the
(combinatorial) computation of 2D Voronoi diagrams. The algorithm is
significantly different from previous ones and some of the involved concepts in
it are in the spirit of linear programming and optics. Parallel implementation
is naturally supported since each Voronoi cell can be computed independently of
the other cells. A new combinatorial structure for representing the cells (and
any convex polytope) is described along the way and the computation of the
induced Delaunay graph is obtained almost automatically.
",2023-07-17,Computer Science - Computational Geometry,http://arxiv.org/abs/1212.1095
On the Convergence Properties of Optimal AdaBoost,"  AdaBoost is one of the most popular ML algorithms. It is simple to implement
and often found very effective by practitioners, while still being
mathematically elegant and theoretically sound. AdaBoost's interesting behavior
in practice still puzzles the ML community. We address the algorithm's
stability and establish multiple convergence properties of ""Optimal AdaBoost,""
a term coined by Rudin, Daubechies, and Schapire in 2004. We prove, in a
reasonably strong computational sense, the almost universal existence of time
averages, and with that, the convergence of the classifier itself, its
generalization error, and its resulting margins, among many other objects, for
fixed data sets under arguably reasonable conditions. Specifically, we frame
Optimal AdaBoost as a dynamical system and, employing tools from ergodic
theory, prove that, under a condition that Optimal AdaBoost does not have ties
for best weak classifier eventually, a condition for which we provide empirical
evidence from high dimensional real-world datasets, the algorithm's update
behaves like a continuous map. We provide constructive proofs of several
arbitrarily accurate approximations of Optimal AdaBoost; prove that they
exhibit certain cycling behavior in finite time, and that the resulting
dynamical system is ergodic; and establish sufficient conditions for the same
to hold for the actual Optimal-AdaBoost update. We believe that our results
provide reasonably strong evidence for the affirmative answer to two open
conjectures, at least from a broad computational-theory perspective: AdaBoost
always cycles and is an ergodic dynamical system. We present empirical evidence
that cycles are hard to detect while time averages stabilize quickly. Our
results ground future convergence-rate analysis and may help optimize
generalization ability and alleviate a practitioner's burden of deciding how
long to run the algorithm.
",2023-01-06,Computer Science - Machine Learning,http://arxiv.org/abs/1212.1108
GNU epsilon -- an extensible programming language,"  Reductionism is a viable strategy for designing and implementing practical
programming languages, leading to solutions which are easier to extend,
experiment with and formally analyze. We formally specify and implement an
extensible programming language, based on a minimalistic first-order imperative
core language plus strong abstraction mechanisms, reflection and
self-modification features. The language can be extended to very high levels:
by using Lisp-style macros and code-to-code transforms which automatically
rewrite high-level expressions into core forms, we define closures and
first-class continuations on top of the core. Non-self-modifying programs can
be analyzed and formally reasoned upon, thanks to the language simple
semantics. We formally develop a static analysis and prove a soundness property
with respect to the dynamic semantics. We develop a parallel garbage collector
suitable to multi-core machines to permit efficient execution of parallel
programs.
",2023-09-27,Computer Science - Programming Languages,http://arxiv.org/abs/1212.5210
Initial Semantics for Reduction Rules,"  We give an algebraic characterization of the syntax and operational semantics
of a class of simply-typed languages, such as the language PCF: we characterize
simply-typed syntax with variable binding and equipped with reduction rules via
a universal property, namely as the initial object of some category of models.
For this purpose, we employ techniques developed in two previous works: in the
first work we model syntactic translations between languages over different
sets of types as initial morphisms in a category of models. In the second work
we characterize untyped syntax with reduction rules as initial object in a
category of models. In the present work, we combine the techniques used earlier
in order to characterize simply-typed syntax with reduction rules as initial
object in a category. The universal property yields an operator which allows to
specify translations---that are semantically faithful by construction---between
languages over possibly different sets of types.
  As an example, we upgrade a translation from PCF to the untyped lambda
calculus, given in previous work, to account for reduction in the source and
target. Specifically, we specify a reduction semantics in the source and target
language through suitable rules. By equipping the untyped lambda calculus with
the structure of a model of PCF, initiality yields a translation from PCF to
the lambda calculus, that is faithful with respect to the reduction semantics
specified by the rules.
  This paper is an extended version of an article published in the proceedings
of WoLLIC 2012.
",2023-06-22,Mathematics - Logic,http://arxiv.org/abs/1212.5668
GESPAR: Efficient Phase Retrieval of Sparse Signals,"  We consider the problem of phase retrieval, namely, recovery of a signal from
the magnitude of its Fourier transform, or of any other linear transform. Due
to the loss of the Fourier phase information, this problem is ill-posed.
Therefore, prior information on the signal is needed in order to enable its
recovery. In this work we consider the case in which the signal is known to be
sparse, i.e., it consists of a small number of nonzero elements in an
appropriate basis. We propose a fast local search method for recovering a
sparse signal from measurements of its Fourier transform (or other linear
transform) magnitude which we refer to as GESPAR: GrEedy Sparse PhAse
Retrieval. Our algorithm does not require matrix lifting, unlike previous
approaches, and therefore is potentially suitable for large scale problems such
as images. Simulation results indicate that GESPAR is fast and more accurate
than existing techniques in a variety of settings.
",2023-07-19,Computer Science - Information Theory,http://arxiv.org/abs/1301.1018
Cramer-Rao Lower Bound and Information Geometry,"  This article focuses on an important piece of work of the world renowned
Indian statistician, Calyampudi Radhakrishna Rao. In 1945, C. R. Rao (25 years
old then) published a pathbreaking paper, which had a profound impact on
subsequent statistical research.
",2023-04-04,Computer Science - Information Theory,http://arxiv.org/abs/1301.3578
"Creative telescoping for rational functions using the Griffiths-Dwork
  method","  Creative telescoping algorithms compute linear differential equations
satisfied by multiple integrals with parameters. We describe a precise and
elementary algorithmic version of the Griffiths-Dwork method for the creative
telescoping of rational functions. This leads to bounds on the order and degree
of the coefficients of the differential equation, and to the first complexity
result which is simply exponential in the number of variables. One of the
important features of the algorithm is that it does not need to compute
certificates. The approach is vindicated by a prototype implementation.
",2023-06-12,Computer Science - Symbolic Computation,http://arxiv.org/abs/1301.4313
"Breathfinding: A Wireless Network that Monitors and Locates Breathing in
  a Home","  This paper explores using RSS measurements on many links in a wireless
network to estimate the breathing rate of a person, and the location where the
breathing is occurring, in a home, while the person is sitting, laying down,
standing, or sleeping. The main challenge in breathing rate estimation is that
""motion interference"", i.e., movements other than a person's breathing,
generally cause larger changes in RSS than inhalation and exhalation. We
develop a method to estimate breathing rate despite motion interference, and
demonstrate its performance during multiple short (3-7 minute) tests and during
a longer 66 minute test. Further, for the same experiments, we show the
location of the breathing person can be estimated, to within about 2 m average
error in a 56 square meter apartment. Being able to locate a breathing person
who is not otherwise moving, without calibration, is important for applications
in search and rescue, health care, and security.
",2023-07-19,Computer Science - Human-Computer Interaction,http://arxiv.org/abs/1302.3820
"Alternating Rate Profile Optimization in Single Stream MIMO Interference
  Channels","  The multiple-input multiple-output interference channel is considered with
perfect channel information at the transmitters and single-user decoding
receivers. With all transmissions restricted to single stream beamforming, we
consider the problem of finding all Pareto optimal rate-tuples in the
achievable rate region. The problem is cast as a rate profile optimization
problem. Due to its nonconvexity, we resort to an alternating approach: For
fixed receivers, optimal transmission is known. For fixed transmitters, we show
that optimal receive beamforming is a solution to an inverse field of values
problem. We prove the solution's stationarity and compare it with existing
approaches.
",2023-07-19,Computer Science - Information Theory,http://arxiv.org/abs/1303.4683
Bicompletions of distance matrices,"  In the practice of information extraction, the input data are usually
arranged into pattern matrices, and analyzed by the methods of linear algebra
and statistics, such as principal component analysis. In some applications, the
tacit assumptions of these methods lead to wrong results. The usual reason is
that the matrix composition of linear algebra presents information as flowing
in waves, whereas it sometimes flows in particles, which seek the shortest
paths. This wave-particle duality in computation and information processing has
been originally observed by Abramsky. In this paper we pursue a particle view
of information, formalized in *distance spaces*, which generalize metric
spaces, but are slightly less general than Lawvere's *generalized metric
spaces*. In this framework, the task of extracting the 'principal components'
from a given matrix of data boils down to a bicompletio}, in the sense of
enriched category theory. We describe the bicompletion construction for
distance matrices. The practical goal that motivates this research is to
develop a method to estimate the hardness of attack constructions in security.
",2023-11-03,Computer Science - Logic in Computer Science,http://arxiv.org/abs/1303.6428
State/event based versus purely Action or State based Logics,"  Although less studied than purely action or state based logics, state/event
based logics are becoming increasingly important. Some systems are best studied
using structures with information on both states and transitions, and it is
these structures over which state/event based logics are defined. The logic
UCTL and its variants are perhaps the most widely studied and implemented of
these logics to date. As yet, however, no-one seems to have defined UCTL*, a
trivial step but a worthwhile one. Here we do just that, but prove in the cases
of both UCTL and UCTL* that these logics are no more expressive than their more
commonplace fragments. Also, acknowledging the importance of modal transition
systems, we define a state/event based logic over a modified modal transition
system as a precursor to further work.
",2023-07-11,Computer Science - Logic in Computer Science,http://arxiv.org/abs/1303.7459
"Operational Concurrency Control in the Face of Arbitrary Scale and
  Latency","  We present for the first time a complete solution to the problem of proving
the correctness of a concurrency control algorithm for collaborative text
editors against the standard consistency model. The success of our approach
stems from the use of comprehensive stringwise operational transformations,
which appear to have escaped a formal treatment until now. Because these
transformations sometimes lead to an increase in the number of operations as
they are transformed, we cannot use inductive methods and adopt the novel idea
of decreasing diagrams instead. We also base our algorithm on a client-server
model rather than a peer-to-peer one, which leads to the correct application of
operational transformations to both newly generated and pending operations. And
lastly we solve the problem of latency, so that our algorithm works perfectly
in practice. The result of these innovations is the first ever formally correct
concurrency control algorithm for collaborative text editors together with a
fast, fault tolerant and highly scalable implementation.
",2023-06-12,Computer Science - Data Structures and Algorithms,http://arxiv.org/abs/1303.7462
"Petition Growth and Success Rates on the UK No. 10 Downing Street
  Website","  Now that so much of collective action takes place online, web-generated data
can further understanding of the mechanics of Internet-based mobilisation. This
trace data offers social science researchers the potential for new forms of
analysis, using real-time transactional data based on entire populations,
rather than sample-based surveys of what people think they did or might do.
This paper uses a `big data' approach to track the growth of over 8,000
petitions to the UK Government on the No. 10 Downing Street website for two
years, analysing the rate of growth per day and testing the hypothesis that the
distribution of daily change will be leptokurtic (rather than normal) as
previous research on agenda setting would suggest. This hypothesis is
confirmed, suggesting that Internet-based mobilisation is characterized by
tipping points (or punctuated equilibria) and explaining some of the volatility
in online collective action. We find also that most successful petitions grow
quickly and that the number of signatures a petition receives on its first day
is a significant factor in explaining the overall number of signatures a
petition receives during its lifetime. These findings have implications for the
strategies of those initiating petitions and the design of web sites with the
aim of maximising citizen engagement with policy issues.
",2023-01-05,Computer Science - Computers and Society,http://arxiv.org/abs/1304.0588
"Temporal Analysis of Activity Patterns of Editors in Collaborative
  Mapping Project of OpenStreetMap","  In the recent years Wikis have become an attractive platform for social
studies of the human behaviour. Containing millions records of edits across the
globe, collaborative systems such as Wikipedia have allowed researchers to gain
a better understanding of editors participation and their activity patterns.
However, contributions made to Geo-wikis_wiki-based collaborative mapping
projects_ differ from systems such as Wikipedia in a fundamental way due to
spatial dimension of the content that limits the contributors to a set of those
who posses local knowledge about a specific area and therefore cross-platform
studies and comparisons are required to build a comprehensive image of online
open collaboration phenomena. In this work, we study the temporal behavioural
pattern of OpenStreetMap editors, a successful example of geo-wiki, for two
European capital cities. We categorise different type of temporal patterns and
report on the historical trend within a period of 7 years of the project age.
We also draw a comparison with the previously observed editing activity
patterns of Wikipedia.
",2023-01-05,Computer Science - Computers and Society,http://arxiv.org/abs/1304.2031
An Empirical Comparison of Three Inference Methods,"  In this paper, an empirical evaluation of three inference methods for
uncertain reasoning is presented in the context of Pathfinder, a large expert
system for the diagnosis of lymph-node pathology. The inference procedures
evaluated are (1) Bayes' theorem, assuming evidence is conditionally
independent given each hypothesis; (2) odds-likelihood updating, assuming
evidence is conditionally independent given each hypothesis and given the
negation of each hypothesis; and (3) a inference method related to the
Dempster-Shafer theory of belief. Both expert-rating and decision-theoretic
metrics are used to compare the diagnostic accuracy of the inference methods.
",2023-01-26,Computer Science - Artificial Intelligence,http://arxiv.org/abs/1304.2357
"Superconvergence Using Pointwise Interpolation in Convection-Diffusion
  Problems","  Considering a singularly perturbed convection-diffusion problem, we present
an analysis for a superconvergence result using pointwise interpolation of
Gau{\ss}-Lobatto type for higher-order streamline diffusion FEM.
  We show a useful connection between two different types of interpolation,
namely a vertex-edge-cell interpolant and a pointwise interpolant. Moreover,
different postprocessing operators are analysed and applied to model problems.
",2023-03-06,Mathematics - Numerical Analysis,http://arxiv.org/abs/1304.7443
"Computing Solution Operators of Boundary-value Problems for Some Linear
  Hyperbolic Systems of PDEs","  We discuss possibilities of application of Numerical Analysis methods to
proving computability, in the sense of the TTE approach, of solution operators
of boundary-value problems for systems of PDEs. We prove computability of the
solution operator for a symmetric hyperbolic system with computable real
coefficients and dissipative boundary conditions, and of the Cauchy problem for
the same system (we also prove computable dependence on the coefficients) in a
cube $Q\subseteq\mathbb R^m$. Such systems describe a wide variety of physical
processes (e.g. elasticity, acoustics, Maxwell equations). Moreover, many
boundary-value problems for the wave equation also can be reduced to this case,
thus we partially answer a question raised in Weihrauch and Zhong (2002).
Compared with most of other existing methods of proving computability for PDEs,
this method does not require existence of explicit solution formulas and is
thus applicable to a broader class of (systems of) equations.
",2023-06-22,Mathematics - Numerical Analysis,http://arxiv.org/abs/1305.2494
"Energy Efficient Transmission over Space Shift Keying Modulated MIMO
  Channels","  Energy-efficient communication using a class of spatial modulation (SM) that
encodes the source information entirely in the antenna indices is considered in
this paper. The energy-efficient modulation design is formulated as a convex
optimization problem, where minimum achievable average symbol power consumption
is derived with rate, performance, and hardware constraints. The theoretical
result bounds any modulation scheme of this class, and encompasses the existing
space shift keying (SSK), generalized SSK (GSSK), and Hamming code-aided SSK
(HSSK) schemes as special cases. The theoretical optimum is achieved by the
proposed practical energy-efficient HSSK (EE-HSSK) scheme that incorporates a
novel use of the Hamming code and Huffman code techniques in the alphabet and
bit-mapping designs. Experimental studies demonstrate that EE-HSSK
significantly outperforms existing schemes in achieving near-optimal energy
efficiency. An analytical exposition of key properties of the existing GSSK
(including SSK) modulation that motivates a fundamental consideration for the
proposed energy-efficient modulation design is also provided.
",2023-07-19,Computer Science - Information Theory,http://arxiv.org/abs/1305.5316
"The most controversial topics in Wikipedia: A multilingual and
  geographical analysis","  We present, visualize and analyse the similarities and differences between
the controversial topics related to ""edit wars"" identified in 10 different
language versions of Wikipedia. After a brief review of the related work we
describe the methods developed to locate, measure, and categorize the
controversial topics in the different languages. Visualizations of the degree
of overlap between the top 100 lists of most controversial articles in
different languages and the content related to geographical locations will be
presented. We discuss what the presented analysis and visualizations can tell
us about the multicultural aspects of Wikipedia and practices of
peer-production. Our results indicate that Wikipedia is more than just an
encyclopaedia; it is also a window into convergent and divergent social-spatial
priorities, interests and preferences.
",2023-01-05,Physics - Physics and Society,http://arxiv.org/abs/1305.5566
Detecting Missing Method Calls as Violations of the Majority Rule,"  When using object-oriented frameworks it is easy to overlook certain
important method calls that are required at particular places in code. In this
paper, we provide a comprehensive set of empirical facts on this problem,
starting from traces of missing method calls in a bug repository. We propose a
new system that searches for missing method calls in software based on the
other method calls that are observable. Our key insight is that the voting
theory concept of majority rule holds for method calls: a call is likely to be
missing if there is a majority of similar pieces of code where this call is
present. The evaluation shows that the system predictions go further missing
method calls and often reveal different kinds of code smells (e.g. violations
of API best practices).
",2023-05-08,Computer Science - Software Engineering,http://arxiv.org/abs/1306.0762
Introduction to Queueing Theory and Stochastic Teletraffic Models,"  The aim of this textbook is to provide students with basic knowledge of
stochastic models that may apply to telecommunications research areas, such as
traffic modelling, resource provisioning and traffic management. These study
areas are often collectively called teletraffic. This book assumes prior
knowledge of a programming language, mathematics, probability and stochastic
processes normally taught in an electrical engineering course. For students who
have some but not sufficiently strong background in probability and stochastic
processes, we provide, in the first few chapters, background on the relevant
concepts in these areas.
",2023-06-13,Mathematics - Probability,http://arxiv.org/abs/1307.2968
"Homology and Bisimulation of Asynchronous Transition Systems and Petri
  Nets","  Homology groups of labelled asynchronous transition systems and Petri nets
are introduced. Examples of computing the homology groups are given. It is
proved that if labelled asynchronous transition systems are bisimulation
equivalent, then they have isomorphic homology groups. A method of constructing
a Petri net with given homology groups is found.
",2023-04-19,Computer Science - Logic in Computer Science,http://arxiv.org/abs/1307.5377
"Does generalization performance of $l^q$ regularization learning depend
  on $q$? A negative example","  $l^q$-regularization has been demonstrated to be an attractive technique in
machine learning and statistical modeling. It attempts to improve the
generalization (prediction) capability of a machine (model) through
appropriately shrinking its coefficients. The shape of a $l^q$ estimator
differs in varying choices of the regularization order $q$. In particular,
$l^1$ leads to the LASSO estimate, while $l^{2}$ corresponds to the smooth
ridge regression. This makes the order $q$ a potential tuning parameter in
applications. To facilitate the use of $l^{q}$-regularization, we intend to
seek for a modeling strategy where an elaborative selection on $q$ is
avoidable. In this spirit, we place our investigation within a general
framework of $l^{q}$-regularized kernel learning under a sample dependent
hypothesis space (SDHS). For a designated class of kernel functions, we show
that all $l^{q}$ estimators for $0< q < \infty$ attain similar generalization
error bounds. These estimated bounds are almost optimal in the sense that up to
a logarithmic factor, the upper and lower bounds are asymptotically identical.
This finding tentatively reveals that, in some modeling contexts, the choice of
$q$ might not have a strong impact in terms of the generalization capability.
From this perspective, $q$ can be arbitrarily specified, or specified merely by
other no generalization criteria like smoothness, computational complexity,
sparsity, etc..
",2023-06-14,Computer Science - Machine Learning,http://arxiv.org/abs/1307.6616
Rapid rise and decay in petition signing,"  Contemporary collective action, much of which involves social media and other
Internet-based platforms, leaves a digital imprint which may be harvested to
better understand the dynamics of mobilization. Petition signing is an example
of collective action which has gained in popularity with rising use of social
media and provides such data for the whole population of petition signatories
for a given platform. This paper tracks the growth curves of all 20,000
petitions to the UK government petitions website
(http://epetitions.direct.gov.uk) and 1,800 petitions to the US White House
site (https://petitions.whitehouse.gov), analyzing the rate of growth and
outreach mechanism. Previous research has suggested the importance of the first
day to the ultimate success of a petition, but has not examined early growth
within that day, made possible here through hourly resolution in the data. The
analysis shows that the vast majority of petitions do not achieve any measure
of success; over 99 percent fail to get the 10,000 signatures required for an
official response and only 0.1 percent attain the 100,000 required for a
parliamentary debate (0.7 percent in the US). We analyze the data through a
multiplicative process model framework to explain the heterogeneous growth of
signatures at the population level. We define and measure an average outreach
factor for petitions and show that it decays very fast (reducing to 0.1 pervent
after 10 hours in the UK and 30 hours in the US). After a day or two, a
petition's fate is virtually set. The findings challenge conventional analyses
of collective action from economics and political science, where the production
function has been assumed to follow an S-shaped curve.
",2023-01-05,Physics - Physics and Society,http://arxiv.org/abs/1308.0239
"Convergence Rates of Distributed Nesterov-like Gradient Methods on
  Random Networks","  We consider distributed optimization in random networks where N nodes
cooperatively minimize the sum \sum_{i=1}^N f_i(x) of their individual convex
costs. Existing literature proposes distributed gradient-like methods that are
computationally cheap and resilient to link failures, but have slow convergence
rates. In this paper, we propose accelerated distributed gradient methods that:
1) are resilient to link failures; 2) computationally cheap; and 3) improve
convergence rates over other gradient methods. We model the network by a
sequence of independent, identically distributed random matrices {W(k)} drawn
from the set of symmetric, stochastic matrices with positive diagonals. The
network is connected on average and the cost functions are convex,
differentiable, with Lipschitz continuous and bounded gradients. We design two
distributed Nesterov-like gradient methods that modify the D-NG and D-NC
methods that we proposed for static networks. We prove their convergence rates
in terms of the expected optimality gap at the cost function. Let k and K be
the number of per-node gradient evaluations and per-node communications,
respectively. Then the modified D-NG achieves rates O(log k/k) and O(\log K/K),
and the modified D-NC rates O(1/k^2) and O(1/K^{2-\xi}), where \xi>0 is
arbitrarily small. For comparison, the standard distributed gradient method
cannot do better than \Omega(1/k^{2/3}) and \Omega(1/K^{2/3}), on the same
class of cost functions (even for static networks). Simulation examples
illustrate our analytical findings.
",2023-07-19,Computer Science - Information Theory,http://arxiv.org/abs/1308.0916
"Gradient Magnitude Similarity Deviation: A Highly Efficient Perceptual
  Image Quality Index","  It is an important task to faithfully evaluate the perceptual quality of
output images in many applications such as image compression, image restoration
and multimedia streaming. A good image quality assessment (IQA) model should
not only deliver high quality prediction accuracy but also be computationally
efficient. The efficiency of IQA metrics is becoming particularly important due
to the increasing proliferation of high-volume visual data in high-speed
networks. We present a new effective and efficient IQA model, called gradient
magnitude similarity deviation (GMSD). The image gradients are sensitive to
image distortions, while different local structures in a distorted image suffer
different degrees of degradations. This motivates us to explore the use of
global variation of gradient based local quality map for overall image quality
prediction. We find that the pixel-wise gradient magnitude similarity (GMS)
between the reference and distorted images combined with a novel pooling
strategy the standard deviation of the GMS map can predict accurately
perceptual image quality. The resulting GMSD algorithm is much faster than most
state-of-the-art IQA methods, and delivers highly competitive prediction
accuracy.
",2023-07-19,Computer Science - Computer Vision and Pattern Recognition,http://arxiv.org/abs/1308.3052
Undecidability of MM-QFAs Language Equivalence Problem,"  Let $L_{>\lambda}(\mathcal{A})$ and $L_{\geq\lambda}(\mathcal{A})$ be the
languages recognized by {\em measure many 1-way quantum finite automata
(MM-QFA)} (or,{\em enhanced 1-way quantum finite automata(EQFA)}) $\mathcal{A}$
with strict and non-strict cut-point $\lambda$, respectively. We consider the
language equivalence problem and show the following
  1. both strict and non-strict language equivalence are undecidable;
  2. we provide an another proof of the undecidability of non-strict and strict
emptiness of MM-QFA and EQFA, and then reducing the language equivalence
problem to emptiness problem;
  3. lastly, we obtain some other properties which can be derived from the
above results.
",2023-06-06,Computer Science - Formal Languages and Automata Theory,http://arxiv.org/abs/1308.3301
Simple and Robust Boolean Operations for Triangulated Surfaces,"  Boolean operations of geometric models is an essential issue in computational
geometry. In this paper, we develop a simple and robust approach to perform
Boolean operations on closed and open triangulated surfaces. Our method mainly
has two stages: (1) We firstly find out candidate intersected-triangles pairs
based on Octree and then compute the inter-section lines for all pairs of
triangles with parallel algorithm; (2) We form closed or open
intersection-loops, sub-surfaces and sub-blocks quite robustly only according
to the cleared and updated topology of meshes while without coordinate
computations for geometric enti-ties. A novel technique instead of
inside/outside classification is also proposed to distinguish the resulting
union, subtraction and intersection. Several examples have been given to
illus-trate the effectiveness of our approach.
",2023-07-19,Computer Science - Computational Geometry,http://arxiv.org/abs/1308.4434
"Matrix Completion in Colocated MIMO Radar: Recoverability, Bounds &
  Theoretical Guarantees","  It was recently shown that low rank matrix completion theory can be employed
for designing new sampling schemes in the context of MIMO radars, which can
lead to the reduction of the high volume of data typically required for
accurate target detection and estimation. Employing random samplers at each
reception antenna, a partially observed version of the received data matrix is
formulated at the fusion center, which, under certain conditions, can be
recovered using convex optimization. This paper presents the theoretical
analysis regarding the performance of matrix completion in colocated MIMO radar
systems, exploiting the particular structure of the data matrix. Both Uniform
Linear Arrays (ULAs) and arbitrary 2-dimensional arrays are considered for
transmission and reception. Especially for the ULA case, under some mild
assumptions on the directions of arrival of the targets, it is explicitly shown
that the coherence of the data matrix is both asymptotically and approximately
optimal with respect to the number of antennas of the arrays involved and
further, the data matrix is recoverable using a subset of its entries with
minimal cardinality. Sufficient conditions guaranteeing low matrix coherence
and consequently satisfactory matrix completion performance are also presented,
including the arbitrary 2-dimensional array case.
",2023-07-19,Computer Science - Information Theory,http://arxiv.org/abs/1308.4994
The Fractal Dimension of SAT Formulas,"  Modern SAT solvers have experienced a remarkable progress on solving
industrial instances. Most of the techniques have been developed after an
intensive experimental testing process. Recently, there have been some attempts
to analyze the structure of these formulas in terms of complex networks, with
the long-term aim of explaining the success of these SAT solving techniques,
and possibly improving them.
  We study the fractal dimension of SAT formulas, and show that most industrial
families of formulas are self-similar, with a small fractal dimension. We also
show that this dimension is not affected by the addition of learnt clauses. We
explore how the dimension of a formula, together with other graph properties
can be used to characterize SAT instances. Finally, we give empirical evidence
that these graph properties can be used in state-of-the-art portfolios.
",2023-03-14,Computer Science - Artificial Intelligence,http://arxiv.org/abs/1308.5046
"Toward an Interactive Directory for Norfolk, Nebraska: 1899-1900","  We describe steps toward an interactive directory for the town of Norfolk,
Nebraska for the years 1899 and 1900. This directory would extend the
traditional city directory by including a wider range of entities being
described, much richer information about the entities mentioned and linkages to
mentions of the entities in material such as digitized historical newspapers.
Such a directory would be useful to readers who browse the historical
newspapers by providing structured summaries of the entities mentioned. We
describe the occurrence of entities in two years of the Norfolk Weekly News,
focusing on several individuals to better understand the types of information
which can be gleaned from historical newspapers and other historical materials.
We also describe a prototype program which coordinates information about
entities from the traditional city directories, the federal census, and from
newspapers. We discuss the structured coding for these entities, noting that
richer coding would increasingly include descriptions of events and scenarios.
We propose that rich content about individuals and communities could eventually
be modeled with agents and woven into historical narratives.
",2023-03-07,Computer Science - Digital Libraries,http://arxiv.org/abs/1308.5395
"Resource Allocation in MIMO Radar With Multiple Targets for Non-Coherent
  Localization","  In a MIMO radar network the multiple transmit elements may emit waveforms
that differ on power and bandwidth. In this paper, we are asking, given that
these two resources are limited, what is the optimal power, optimal bandwidth
and optimal joint power and bandwidth allocation for best localization of
multiple targets. The well known Cr\'amer-Rao lower bound for target
localization accuracy is used as a figure of merit and approximate solutions
are found by minimizing a sequence of convex problems. Their quality is
assessed through extensive numerical simulations and with the help of a
lower-bound on the true solution. Simulations results reveal that bandwidth
allocation policies have a definitely stronger impact on performance than
power.
",2023-07-19,Computer Science - Information Theory,http://arxiv.org/abs/1308.6543
Sigma Point Belief Propagation,"  The sigma point (SP) filter, also known as unscented Kalman filter, is an
attractive alternative to the extended Kalman filter and the particle filter.
Here, we extend the SP filter to nonsequential Bayesian inference corresponding
to loopy factor graphs. We propose sigma point belief propagation (SPBP) as a
low-complexity approximation of the belief propagation (BP) message passing
scheme. SPBP achieves approximate marginalizations of posterior distributions
corresponding to (generally) loopy factor graphs. It is well suited for
decentralized inference because of its low communication requirements. For a
decentralized, dynamic sensor localization problem, we demonstrate that SPBP
can outperform nonparametric (particle-based) BP while requiring significantly
less computations and communications.
",2023-07-19,Computer Science - Artificial Intelligence,http://arxiv.org/abs/1309.0363
"Application of Expurgated PPM to Indoor Visible Light Communications -
  Part I: Single-User Systems","  Visible light communications (VLC) in indoor environments suffer from the
limited bandwidth of LEDs as well as from the inter-symbol interference (ISI)
imposed by multipath. In this work, transmission schemes to improve the
performance of indoor optical wireless communication (OWC) systems are
introduced. Expurgated pulse-position modulation (EPPM) is proposed for this
application since it can provide a wide range of peak to average power ratios
(PAPR) needed for dimming of the indoor illumination. A correlation decoder
used at the receiver is shown to be optimal for indoor VLC systems, which are
shot noise and background-light limited. Interleaving applied on EPPM in order
to decrease the ISI effect in dispersive VLC channels can significantly
decrease the error probability. The proposed interleaving technique makes EPPM
a better modulation option compared to PPM for VLC systems or any other
dispersive OWC system. An overlapped EPPM pulse technique is proposed to
increase the transmission rate when bandwidth-limited white LEDs are used as
sources.
",2023-07-19,Computer Science - Information Theory,http://arxiv.org/abs/1309.0750
"Application of Expurgated PPM to Indoor Visible Light Communications -
  Part II: Access Networks","  Providing network access for multiple users in a visible light communication
(VLC) system that utilizes white light emitting diodes (LED) as sources
requires new networking techniques adapted to the lighting features. In this
paper we introduce two multiple access techniques using expurgated PPM (EPPM)
that can be implemented using LEDs and support lighting features such as
dimming. Multilevel symbols are used to provide M-ary signaling for multiple
users using multilevel EPPM (MEPPM). Using these multiple-access schemes we are
able to control the optical peak to average power ratio (PAPR) in the system,
and hereby control the dimming level. In the first technique, the M-ary data of
each user is first encoded using an optical orthogonal code (OOC) assigned to
the user, and the result is fed into a EPPM encoder to generate a multilevel
signal. The second multiple access method uses sub-sets of the EPPM
constellation to apply MEPPM to the data of each user. While the first approach
has a larger Hamming distance between the symbols of each user, the latter can
provide higher bit-rates for users in VLC systems using bandwidth-limited LEDs.
",2023-07-19,Computer Science - Information Theory,http://arxiv.org/abs/1309.0775
On topological and geometric $(19_4)$ configurations,"  An $(n_k)$ configuration is a set of $n$ points and $n$ lines such that each
point lies on $k$ lines while each line contains $k$ points. The configuration
is geometric, topological, or combinatorial depending on whether lines are
considered to be straight lines, pseudolines, or just combinatorial lines. The
existence and enumeration of $(n_k)$ configurations for a given $k$ has been
subject to active research. A current front of research concerns geometric
$(n_4)$ configurations: it is now known that geometric $(n_4)$ configurations
exist for all $n \ge 18$, apart from sporadic exceptional cases. In this paper,
we settle by computational techniques the first open case of $(19_4)$
configurations: we obtain all topological $(19_4)$ configurations among which
none are geometrically realizable.
",2023-11-14,Computer Science - Computational Geometry,http://arxiv.org/abs/1309.3201
"Numerical solutions of a class of second order boundary value problems
  on using Bernoulli Polynomials","  The aim of this paper is to find the numerical solutions of the second order
linear and nonlinear differential equations with Dirichlet, Neumann and Robin
boundary conditions. We use the Bernoulli polynomials as linear combination to
the approximate solutions of 2nd order boundary value problems. Here the
Bernoulli polynomials over the interval [0, 1] are chosen as trial functions so
that care has been taken to satisfy the corresponding homogeneous form of the
Dirichlet boundary conditions in the Galerkin weighted residual method. In
addition to that the given differential equation over arbitrary finite domain
[a, b] and the boundary conditions are converted into its equivalent form over
the interval [0, 1]. All the formulas are verified by considering numerical
examples. The approximate solutions are compared with the exact solutions, and
also with the solutions of the existing methods. A reliable good accuracy is
obtained in all cases.
",2023-05-31,Mathematics - Numerical Analysis,http://arxiv.org/abs/1309.6064
"Graphs of Edge-Intersecting Non-Splitting Paths in a Tree:
  Representations of Holes-Part II","  Given a tree and a set P of non-trivial simple paths on it, VPT(P) is the VPT
graph (i.e. the vertex intersection graph) of the paths P, and EPT(P) is the
EPT graph (i.e. the edge intersection graph) of P. These graphs have been
extensively studied in the literature. Given two (edge) intersecting paths in a
graph, their split vertices is the set of vertices having degree at least 3 in
their union. A pair of (edge) intersecting paths is termed non-splitting if
they do not have split vertices (namely if their union is a path). We define
the graph ENPT(P) of edge intersecting non-splitting paths of a tree, termed
the ENPT graph, as the graph having a vertex for each path in P, and an edge
between every pair of vertices representing two paths that are both
edge-intersecting and non-splitting. A graph G is an ENPT graph if there is a
tree T and a set of paths P of T such that G=ENPT(P), and we say that <T,P> is
a representation of G.
  Our goal is to characterize the representation of chordless ENPT cycles
(holes). To achieve this goal, we first assume that the EPT graph induced by
the vertices of an ENPT hole is given. In [2] we introduce three assumptions
(P1), (P2), (P3) defined on EPT, ENPT pairs of graphs. In the same study, we
define two problems HamiltonianPairRec, P3-HamiltonianPairRec and characterize
the representations of ENPT holes that satisfy (P1), (P2), (P3).
  In this work, we continue our work by relaxing these three assumptions one by
one. We characterize the representations of ENPT holes satisfying (P3) by
providing a polynomial-time algorithm to solve P3-HamiltonianPairRec. We also
show that there does not exist a polynomial-time algorithm to solve
HamiltonianPairRec, unless P=NP.
",2023-06-22,Computer Science - Discrete Mathematics,http://arxiv.org/abs/1309.6471
"Multiuser Diversity for Secrecy Communications Using Opportunistic
  Jammer Selection -- Secure DoF and Jammer Scaling Law","  In this paper, we propose opportunistic jammer selection in a wireless
security system for increasing the secure degrees of freedom (DoF) between a
transmitter and a legitimate receiver (say, Alice and Bob). There is a jammer
group consisting of $S$ jammers among which Bob selects $K$ jammers. The
selected jammers transmit independent and identically distributed Gaussian
signals to hinder the eavesdropper (Eve). Since the channels of Bob and Eve are
independent, we can select the jammers whose jamming channels are aligned at
Bob, but not at Eve. As a result, Eve cannot obtain any DoF unless it has more
than $KN_j$ receive antennas, where $N_j$ is the number of jammer's transmit
antenna each, and hence $KN_j$ can be regarded as defensible dimensions against
Eve. For the jamming signal alignment at Bob, we propose two opportunistic
jammer selection schemes and find the scaling law of the required number of
jammers for target secure DoF by a geometrical interpretation of the received
signals.
",2023-07-19,Computer Science - Information Theory,http://arxiv.org/abs/1309.7451
When does a physical system compute?,"  Computing is a high-level process of a physical system. Recent interest in
non-standard computing systems, including quantum and biological computers, has
brought this physical basis of computing to the forefront. There has been,
however, no consensus on how to tell if a given physical system is acting as a
computer or not; leading to confusion over novel computational devices, and
even claims that every physical event is a computation. In this paper we
introduce a formal framework that can be used to determine whether or not a
physical system is performing a computation. We demonstrate how the abstract
computational level interacts with the physical device level, drawing the
comparison with the use of mathematical models to represent physical objects in
experimental science. This powerful formulation allows a precise description of
the similarities between experiments, computation, simulation, and technology,
leading to our central conclusion: physical computing is the use of a physical
system to predict the outcome of an abstract evolution. We give conditions that
must be satisfied in order for computation to be occurring, and illustrate
these with a range of non-standard computing scenarios. The framework also
covers broader computing contexts, where there is no obvious human computer
user. We define the critical notion of a 'computational entity', and show the
role this plays in defining when computing is taking place in physical systems.
",2023-04-21,Computer Science - Emerging Technologies,http://arxiv.org/abs/1309.7979
SVD Factorization for Tall-and-Fat Matrices on Parallel Architectures,"  We demonstrate an implementation for an approximate rank-k SVD factorization,
combining well-known randomized projection techniques with previously known
paralel solutions in order to compute steps of the random projection based SVD
procedure. We structure the problem in a way that it reduces to fast
computation around $k \times k$ matrices computed on a single machine, greatly
easing the computability of the problem. The paper is also a tutorial on
paralel linear algebra methods using a plain architecture without burdensome
frameworks.
",2023-04-04,"Computer Science - Distributed, Parallel, and Cluster Computing",http://arxiv.org/abs/1310.4664
Can social microblogging be used to forecast intraday exchange rates?,"  The Efficient Market Hypothesis (EMH) is widely accepted to hold true under
certain assumptions. One of its implications is that the prediction of stock
prices at least in the short run cannot outperform the random walk model. Yet,
recently many studies stressing the psychological and social dimension of
financial behavior have challenged the validity of the EMH. Towards this aim,
over the last few years, internet-based communication platforms and search
engines have been used to extract early indicators of social and economic
trends. Here, we used Twitter's social networking platform to model and
forecast the EUR/USD exchange rate in a high-frequency intradaily trading
scale. Using time series and trading simulations analysis, we provide some
evidence that the information provided in social microblogging platforms such
as Twitter can in certain cases enhance the forecasting efficiency regarding
the very short (intradaily) forex.
",2023-03-24,Computer Science - Social and Information Networks,http://arxiv.org/abs/1310.5306
