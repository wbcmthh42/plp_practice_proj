{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.16.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Downloading numpy-2.1.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.11.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets)\n",
      "  Downloading idna-3.8-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading aiohttp-3.10.5-cp312-cp312-macosx_11_0_arm64.whl (389 kB)\n",
      "Downloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Downloading numpy-2.1.1-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl (27.2 MB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.16.0-py3-none-any.whl (16 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Downloading idna-3.8-py3-none-any.whl (66 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Downloading yarl-1.11.1-cp312-cp312-macosx_11_0_arm64.whl (112 kB)\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, tqdm, pyyaml, numpy, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, pyarrow, pandas, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.3.2 datasets-2.21.0 dill-0.3.8 filelock-3.16.0 frozenlist-1.4.1 fsspec-2024.6.1 huggingface-hub-0.24.6 idna-3.8 multidict-6.1.0 multiprocess-0.70.16 numpy-2.1.1 pandas-2.2.2 pyarrow-17.0.0 pytz-2024.2 pyyaml-6.0.2 requests-2.32.3 tqdm-4.66.5 tzdata-2024.1 urllib3-2.2.2 xxhash-3.5.0 yarl-1.11.1\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from accelerate) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Collecting torch>=1.10.0 (from accelerate)\n",
      "  Downloading torch-2.4.1-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from accelerate) (0.24.6)\n",
      "Collecting safetensors>=0.4.3 (from accelerate)\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Collecting sympy (from torch>=1.10.0->accelerate)\n",
      "  Using cached sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.10.0->accelerate)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch>=1.10.0->accelerate)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (72.1.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.10.0->accelerate)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.10.0->accelerate)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "Downloading safetensors-0.4.5-cp312-cp312-macosx_11_0_arm64.whl (381 kB)\n",
      "Downloading torch-2.4.1-cp312-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, safetensors, networkx, MarkupSafe, jinja2, torch, accelerate\n",
      "Successfully installed MarkupSafe-2.1.5 accelerate-0.34.2 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 safetensors-0.4.5 sympy-1.13.2 torch-2.4.1\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from transformers) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.7.24-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2024.7.24-cp312-cp312-macosx_11_0_arm64.whl (279 kB)\n",
      "Downloading tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.7.24 tokenizers-0.19.1 transformers-4.44.2\n",
      "Requirement already satisfied: huggingface_hub in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (0.24.6)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface_hub) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from requests->huggingface_hub) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/plp_module/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets\n",
    "!pip install -U accelerate\n",
    "!pip install -U transformers\n",
    "!pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/plp_module/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 888/888 [00:00<00:00, 3.38kB/s]\n",
      "Downloading data: 100%|██████████| 2.06M/2.06M [00:01<00:00, 1.47MB/s]\n",
      "Downloading data: 100%|██████████| 186k/186k [00:00<00:00, 237kB/s]\n",
      "Downloading data: 100%|██████████| 190k/190k [00:00<00:00, 283kB/s]\n",
      "Generating train split: 100%|██████████| 7196/7196 [00:00<00:00, 410798.83 examples/s]\n",
      "Generating validation split: 100%|██████████| 635/635 [00:00<00:00, 281095.84 examples/s]\n",
      "Generating test split: 100%|██████████| 635/635 [00:00<00:00, 350214.73 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'source', 'text', 'timestamp', 'reactions', 'engagement', 'url', 'text_length', 'keywords', 'topic', 'summary', '__index_level_0__'],\n",
       "        num_rows: 7196\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'source', 'text', 'timestamp', 'reactions', 'engagement', 'url', 'text_length', 'keywords', 'topic', 'summary', '__index_level_0__'],\n",
       "        num_rows: 635\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'source', 'text', 'timestamp', 'reactions', 'engagement', 'url', 'text_length', 'keywords', 'topic', 'summary', '__index_level_0__'],\n",
       "        num_rows: 635\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ilsilfverskiold/tech-keywords-topics-summary\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>> Text: Driverless car users will not be prosecuted for fatal crashes in UK'\n",
      "'>> Keywords: Driverless Cars, Legal Issues, UK'\n",
      "\n",
      "'>> Text: Google is embedding inaudible watermarks right into its AI generated music -'\n",
      "'>> Keywords: Google, AI Music, Watermarks, Audio Technology'\n",
      "\n",
      "'>> Text: What are your thoughts on Nextjs performance? Do you agree with this chart? - ( by 10up where Nextjs appears lower than WordPress on core vitals. Couldn’t post the image here due to community rules. But appreciate any other studies and thought you have on this matter.'\n",
      "'>> Keywords: Next.js, Performance, 10up, WordPress'\n"
     ]
    }
   ],
   "source": [
    "def show_samples(dataset, num_samples=3, seed=42):\n",
    "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
    "    for example in sample:\n",
    "        print(f\"\\n'>> Text: {example['text']}'\")\n",
    "        print(f\"'>> Keywords: {example['keywords']}'\")\n",
    "\n",
    "\n",
    "show_samples(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/plp_module/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest text is 245 tokens long.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = 'facebook/bart-large' # go smaller if you can\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "texts = dataset['train']['text']\n",
    "\n",
    "# Tokenize all texts and find the maximum length (max for BART is 1024 tokens)\n",
    "max_token_length = max(len(tokenizer.encode(text, truncation=True)) for text in texts)\n",
    "print(f\"The longest text is {max_token_length} tokens long.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7196/7196 [00:00<00:00, 25095.57 examples/s]\n",
      "Map: 100%|██████████| 635/635 [00:00<00:00, 22322.28 examples/s]\n",
      "Map: 100%|██████████| 635/635 [00:00<00:00, 22430.94 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'source', 'text', 'timestamp', 'reactions', 'engagement', 'url', 'text_length', 'keywords', 'topic', 'summary', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7196\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'source', 'text', 'timestamp', 'reactions', 'engagement', 'url', 'text_length', 'keywords', 'topic', 'summary', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 635\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'source', 'text', 'timestamp', 'reactions', 'engagement', 'url', 'text_length', 'keywords', 'topic', 'summary', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 635\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_feature(batch):\n",
    "  encodings = tokenizer(batch['text'], text_target=batch['keywords'],\n",
    "                        max_length=1024, truncation=True)\n",
    "\n",
    "  encodings = {'input_ids': encodings['input_ids'],\n",
    "               'attention_mask': encodings['attention_mask'],\n",
    "               'labels': encodings['labels']}\n",
    "\n",
    "  return encodings\n",
    "\n",
    "dataset_pt = dataset.map(get_feature, batched=True)\n",
    "dataset_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['input_ids', 'labels', 'attention_mask']\n",
    "dataset_pt.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/plp_module/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/336 [00:00<?, ?it/s]/opt/miniconda3/envs/plp_module/lib/python3.12/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "  3%|▎         | 10/336 [01:20<37:17,  6.86s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6513, 'grad_norm': 40.85301971435547, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 20/336 [02:41<45:16,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1626, 'grad_norm': 31.181283950805664, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 30/336 [03:56<35:56,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3209, 'grad_norm': 20.88770866394043, 'learning_rate': 3e-06, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 40/336 [05:01<31:08,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7205, 'grad_norm': 10.746589660644531, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 50/336 [06:26<45:20,  9.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4049, 'grad_norm': 10.60523509979248, 'learning_rate': 5e-06, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 15%|█▍        | 50/336 [07:27<45:20,  9.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1728525161743164, 'eval_runtime': 60.2591, 'eval_samples_per_second': 10.538, 'eval_steps_per_second': 2.639, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 60/336 [09:01<45:50,  9.97s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3121, 'grad_norm': 7.623536586761475, 'learning_rate': 6e-06, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 70/336 [10:34<40:46,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2611, 'grad_norm': 10.073939323425293, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 80/336 [12:34<1:11:13, 16.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2297, 'grad_norm': 8.070141792297363, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 90/336 [36:51<17:12:55, 251.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1682, 'grad_norm': 13.98412036895752, 'learning_rate': 9e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 100/336 [1:26:10<6:15:45, 95.53s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1023, 'grad_norm': 8.362699508666992, 'learning_rate': 1e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 30%|██▉       | 100/336 [1:27:04<6:15:45, 95.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0794211626052856, 'eval_runtime': 54.5605, 'eval_samples_per_second': 11.638, 'eval_steps_per_second': 2.914, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 110/336 [1:28:45<47:32, 12.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0717, 'grad_norm': 13.899110794067383, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 120/336 [1:30:14<31:15,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0577, 'grad_norm': 6.83284854888916, 'learning_rate': 1.2e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 130/336 [1:31:33<27:28,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0065, 'grad_norm': 12.647035598754883, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 140/336 [1:32:52<25:47,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0244, 'grad_norm': 7.274231910705566, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 150/336 [1:34:43<40:22, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9659, 'grad_norm': 11.281682014465332, 'learning_rate': 1.5e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 45%|████▍     | 150/336 [1:36:01<40:22, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9492010474205017, 'eval_runtime': 78.0204, 'eval_samples_per_second': 8.139, 'eval_steps_per_second': 2.038, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 160/336 [1:37:43<32:13, 10.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9744, 'grad_norm': 9.96319580078125, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 170/336 [1:39:05<23:31,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9483, 'grad_norm': 10.527077674865723, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 180/336 [1:40:30<22:40,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9721, 'grad_norm': 6.891709804534912, 'learning_rate': 1.8e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 190/336 [1:41:53<20:05,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9142, 'grad_norm': 10.293627738952637, 'learning_rate': 1.9e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 200/336 [1:43:16<18:33,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9226, 'grad_norm': 12.827656745910645, 'learning_rate': 2e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 200/336 [1:44:13<18:33,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8965096473693848, 'eval_runtime': 57.0921, 'eval_samples_per_second': 11.122, 'eval_steps_per_second': 2.785, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 210/336 [1:45:33<18:04,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9299, 'grad_norm': 8.831805229187012, 'learning_rate': 2.1e-05, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 220/336 [1:46:57<16:35,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9575, 'grad_norm': 14.106292724609375, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 230/336 [1:48:24<15:15,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8384, 'grad_norm': 6.936272144317627, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 240/336 [1:49:48<13:53,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8157, 'grad_norm': 10.196778297424316, 'learning_rate': 2.4e-05, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 250/336 [1:51:12<12:10,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8488, 'grad_norm': 7.5270676612854, 'learning_rate': 2.5e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 74%|███████▍  | 250/336 [1:52:13<12:10,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8998113870620728, 'eval_runtime': 60.4211, 'eval_samples_per_second': 10.51, 'eval_steps_per_second': 2.632, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 260/336 [1:53:39<11:53,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8173, 'grad_norm': 8.18798828125, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 270/336 [1:55:06<09:08,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8219, 'grad_norm': 7.067136287689209, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 280/336 [1:56:35<08:25,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8307, 'grad_norm': 6.97703218460083, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 290/336 [1:58:04<06:49,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8388, 'grad_norm': 9.943134307861328, 'learning_rate': 2.9e-05, 'epoch': 2.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 300/336 [1:59:34<05:28,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8581, 'grad_norm': 6.239572525024414, 'learning_rate': 3e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 89%|████████▉ | 300/336 [2:00:37<05:28,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8912317156791687, 'eval_runtime': 62.9613, 'eval_samples_per_second': 10.086, 'eval_steps_per_second': 2.525, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 310/336 [2:02:21<05:09, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8445, 'grad_norm': 10.027575492858887, 'learning_rate': 3.1e-05, 'epoch': 2.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 320/336 [2:11:29<19:59, 74.94s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.817, 'grad_norm': 4.917409420013428, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 330/336 [2:13:27<01:21, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8061, 'grad_norm': 10.157153129577637, 'learning_rate': 3.3e-05, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336/336 [2:14:41<00:00, 12.53s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "100%|██████████| 336/336 [2:14:45<00:00, 24.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8085.979, 'train_samples_per_second': 2.67, 'train_steps_per_second': 0.042, 'train_loss': 1.1810229207788194, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=336, training_loss=1.1810229207788194, metrics={'train_runtime': 8085.979, 'train_samples_per_second': 2.67, 'train_steps_per_second': 0.042, 'total_flos': 3582300094562304.0, 'train_loss': 1.1810229207788194, 'epoch': 2.9883268482490273})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'bart_tech_keywords', # rename to what you want it to be called\n",
    "    num_train_epochs=3, # your choice\n",
    "    warmup_steps = 500,\n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay = 0.01,\n",
    "    logging_steps = 10,\n",
    "    evaluation_strategy = 'steps',\n",
    "    eval_steps=50, \n",
    "    save_steps=1e6,\n",
    "    gradient_accumulation_steps=16 \n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer, data_collator=data_collator,\n",
    "                  train_dataset = dataset_pt['train'], eval_dataset = dataset_pt['validation'])\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('tech-keywords-extractor_plp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Your max_length is set to 128, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the text:  Apple is launching iphone 16 with Apple Intelligence. It also uses OpenAI ChatGPT\n",
      "generated keywords:  [{'summary_text': 'Apple, iphone 16, Apple Intelligence, OpenAI, ChatGPT'}]\n"
     ]
    }
   ],
   "source": [
    "# !huggingface-cli login --token hf_xxxxxxxxxxxx\n",
    "\n",
    "# check output of fine tuned model\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('summarization', model='tech-keywords-extractor_plp')\n",
    "\n",
    "text_test = \"Apple is launching iphone 16 with Apple Intelligence. It also uses OpenAI ChatGPT\"\n",
    "\n",
    "# test_text=dataset['test'][0]['text']\n",
    "# keywords = dataset['test'][0]['keywords']\n",
    "print(\"the text: \", text_test)\n",
    "print(\"generated keywords: \", pipe(text_test))\n",
    "# print(\"orginal keywords : \",keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Your max_length is set to 128, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the text:  Apple is launching iphone 16 with Apple Intelligence. It also uses OpenAI ChatGPT. \n",
      "generated keywords:  [{'summary_text': 'Apple is launching iphone 16 with Apple Intelligence. It also uses OpenAI ChatGPT. '}]\n"
     ]
    }
   ],
   "source": [
    "# Check back on output of base untrained model\n",
    "\n",
    "pipe = pipeline('summarization', model = 'facebook/bart-large')\n",
    "\n",
    "text_test = \"Apple is launching iphone 16 with Apple Intelligence. It also uses OpenAI ChatGPT. \"\n",
    "\n",
    "# test_text=dataset['test'][0]['text']\n",
    "# keywords = dataset['test'][0]['keywords']\n",
    "print(\"the text: \", text_test)\n",
    "print(\"generated keywords: \", pipe(text_test))\n",
    "# print(\"orginal keywords : \",keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Your max_length is set to 142, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the text:  Apple is launching iphone 16 with Apple Intelligence. It also uses OpenAI ChatGPT. \n",
      "generated keywords:  [{'summary_text': \"Apple is launching iphone 16 with Apple Intelligence. It also uses OpenAI ChatGPT. Apple is launching Apple Intelligence with Apple Artificial Intelligence on the iPhone 16. Apple  also using OpenAI chatGPT on the phone. Apple's new iPhone 16 will be released on September 18.\"}]\n"
     ]
    }
   ],
   "source": [
    "# Check back on output of base model fine tuned on CNN daily news\n",
    "\n",
    "pipe = pipeline('summarization', model = 'facebook/bart-large-cnn')\n",
    "\n",
    "text_test = \"Apple is launching iphone 16 with Apple Intelligence. It also uses OpenAI ChatGPT. \"\n",
    "\n",
    "# test_text=dataset['test'][0]['text']\n",
    "# keywords = dataset['test'][0]['keywords']\n",
    "print(\"the text: \", text_test)\n",
    "print(\"generated keywords: \", pipe(text_test))\n",
    "# print(\"orginal keywords : \",keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  Unleash the Edge: Women’s Faux Leather Zip-Up Motorcycle Jacket — Vintage Grunge Aesthetic Revived - Ride the Rebel Wave: Women’s Faux Leather Zip-Up Motorcycle Jacket — A Fusion of Timeless Edge and Vintage Grunge Aesthetic for…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 12. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Women's Faux Leather Zip-Up Motorcycle Jacket, Vintage Grunge Aesthetic\n",
      "original keywords:  Women's Fashion, Faux Leather, Motorcycle Jacket, Vintage Grunge\n",
      "text:  Distractions, analytical thinking and falling for fake news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 20. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Distractions, analytical thinking, Fake News\n",
      "original keywords:  Distractions, Analytical Thinking, Fake News\n",
      "text:  If you buy a Cybertruck, Tesla says you can't sell it for a year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Cybertruck, Tesla, Auto Sales\n",
      "original keywords:  Cybertruck, Tesla, Resale Restriction\n",
      "text:  ODROID-M1S is a $49 single-board PC with RK3566, 64GB eMMC and an M.2 2280 slot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  ODROID-M1S, Single-board PC, RK3566, eMMC, M.2 2280\n",
      "original keywords:  ODROID-M1S, RK3566, Single-Board PC, M.2 Slot\n",
      "text:  ChatGPT-Admin-Web - One-stop system for shared use of AI within teams and organizationswith |  - English ChatGPT Admin Web CAW GitHub Sponsor / Features V3 V2 + topics: ai-system, chatgpt, gpt-4, llm, llvm, newbing, nextjs, postgresql, prisma, prompt, user-management, webui\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 28. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  ChatGPT, Admin Web, GitHub, AI, Teams, Organizations\n",
      "original keywords:  ChatGPT-Admin-Web, AI, ChatGPT, GPT-4, LLM, LLVM, Newbing, Next.js, PostgreSQL, Prisma, User Management, WebUI\n",
      "text:  Microsoft Azure CTO Headhunted for SDE II Position at Amazontwitter.com/markrussinovich\n",
      "generated keywords:  Microsoft Azure, CTO, SDE II, Amazon\n",
      "original keywords:  Microsoft Azure, Microsoft, Azure, CTO, Amazon, Mark Russinovich\n",
      "text:  Self-Hosting-Guide - Self-Hosting Guide. Learn all about locally hosting  and managing software applications by yourself or your organization. Including Cloud, LLMs, WireGuard, Automation, Home Assistant, and Networking. - Self Hosting Guide A guide for getting started with Self Hosting devices including software and hardware that will make you a better and more efficient Self Hosting. Note: You can easily convert this markdown file to a PDF in VSCode using this handy extension Markdown PDF . Note 2: This guide will constantly be updated with new info as becomes available and please feel to make an issue if you think something should be added + topics: authentication, awesome, awesome-list, decentralized, docker-compose, home-assistant, home-automation, linux, oauth, observability, open-source, privacy, raspberry-pi, reverse-proxy, search, self-hosted, self-hosting, selfhosted, ssh, wireguard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Self Hosting, Cloud, LLMs, WireGuard, Automation, Home Assistant, Home Automation\n",
      "original keywords:  Self Hosting, Cloud, LLMs, WireGuard, Automation, Home Assistant, Networking, Authentication, Docker, Docker Compose, Linux, OAuth, Observability, Open Source, Privacy, Raspberry Pi, Reverse Proxy, Search, SSH\n",
      "text:  Pendulation is normal - Think of a pendulum. When you push it in one direction, it swings back with opposite force.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Pendulum, Normal Motion, Physics\n",
      "original keywords:  Pendulation, Pendulum\n",
      "text:  ChatGPT Voice for Medium Article Writers and Content Creators - Unleash the Conversation with AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 62. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  ChatGPT Voice, Medium, Article Writers, Content Creators, AI\n",
      "original keywords:  ChatGPT, Voice, Medium, Content Creation\n",
      "text:  CookieCloud - CookieCloudCookieCookieLocal storage - CookieCloudCookieCloudCookieCookieLocal storagelocal storageTelegram | Telegram Breaking Change local storage 0.1.5+ cookie local storagecookie{ cookie_data, local_storage_data } remote local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  CookieCloud, Telegram, Local Storage, Storagelocal\n",
      "original keywords:  CookieCloud, Local Storage, Telegram, Breaking Change\n",
      "text:  Essential Resources to Learn App Development for Apple Vision Pro -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 112. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=56)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  App Development, Apple Vision Pro, Apple\n",
      "original keywords:  App Development, Apple Vision Pro\n",
      "text:  developer-roadmap - Interactive roadmaps, guides and other educational content to help developers grow in their careers. - roadmap.sh Community driven roadmaps, articles and resources for developers Roadmaps are now interactive, you can click the nodes to read more about the topics.View all Roadmaps Best Practices QuestionsHere is the list of available roadmaps with more being actively worked upon.Frontend Roadmap / Frontend Beginner RoadmapBackend RoadmapDevOps Roadmap / DevOps Beginner RoadmapFull Stack RoadmapComputer Scie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 115. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  developer-roadmap, Roadmaps, Guides, Educational Content, DevOps\n",
      "original keywords:  developer-roadmap, Developer Roadmap, Interactive Roadmaps, Frontend Roadmap, Backend Roadmap, DevOps Roadmap, Full Stack Roadmap, Computer Science, Road Map\n",
      "text:  Leaflet - JavaScript library for mobile-friendly interactive maps - Leaflet was created 11 years ago by Volodymyr Agafonkin, a Ukrainian citizen living in Kyiv.Russian bombs are now falling over Volodymyr's hometown. His family, his friends, his neighbours, thousands and thousands of absolutely wonderful people, are either seeking refuge or fighting for their lives.Russian soldiers have already killed tens of thousands of civilians, including women and children, and are committing mass war crimes like gang rapes, executions, looting, and targeted bombings of c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Leaflet, JavaScript, Mobile-Friendly, Interactive Maps, Ukrainian Citizen, Ukraine, Russian Bombs\n",
      "original keywords:  Leaflet, JavaScript, Interactive Maps, Volodymyr Agafonkin, Ukraine, Kyiv\n",
      "text:  We Are In A Simulation | The Game Of Life Explained - “You are either the main character, a supporting actor, or an extra. Which one are you?”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 54. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Simulation, Game Of Life, Supporting Actor, Extra\n",
      "original keywords:  Simulation Theory, Game of Life\n",
      "text:  OpenAI Dev Day lo más relevante - El 6 de Noviembre se llevó a cabo el primer OpenAI Dev Day en San Francisco, y como era de esperarse, se presentaron grandes sorpresas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  , OpenAI Dev Day, San Francisco\n",
      "original keywords:  OpenAI, DevDay, San Francisco\n",
      "text:  Top 10 Programming Memes Every Developer Can Relate To - If you want to receive an exclusive programming meme every Monday to your inbox, sign up for my newsletter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Programming Memes, Developer, Newsletter\n",
      "original keywords:  Programming Memes, Developer, Newsletter\n",
      "text:  Is sessionStorage useful in React applications? - Why would you ever save data to sessionStorage when you could just store it in the React application in the form of e.g. globalState ? Any benefits to storing data in sessionStorage in React?\n",
      "generated keywords:  SessionStorage, React, Data Storage, GlobalState\n",
      "original keywords:  sessionStorage, React, Global State\n",
      "text:  Worth suing co-founders? - Took an equity position as technical co-founder plus cash incentive and they don’t want to pay now. Contract is a bit of mess on top of it. I’m owed multiple 6 figures for promised sweat equity, but the company isn’t generating what anyone thought it would. There was never a contingency based on success of the company or revenue generated. My choices are basically sue, or try to dissolve company and negotiate to keep all rights to all digital assets, trademarks, domain, etc. I’ve spoken to many lawyers and all have different opinions. I initially thought, hell yes it’s worth it to sue for $250k owed to me. But, it’s really not looking to be a good option after all. The problem is I have little interest in the industry even if I did retain all IP to myself. Kind of a conundrum. Would love to hear some perspectives from others who have faced similar circumstances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 30. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Technical Co-founder, Lawsuit, Digital Assets, Trademarks, Domain, IP, Legal Issues\n",
      "original keywords:  Equity, Technical Co-founder, founder, Contract, Sue, Intellectual Property, IP, Trademarks, Domain\n",
      "text:  No. Americans Runners Have NOT Never Been Slower - A Critique of Run Repeat’s 2017 Mega Study on Marathons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Americans Runners, Marathons, Run Repeat, Study\n",
      "original keywords:  American Runners, Run Repeat, Marathons, Study Critique\n",
      "text:  spectest - API testing library for Go that generate E2E test result document in markdown - Generate document from end-to-end tests In recent years, Domain-Driven Design  has become mainstream, making it easier to implement unit tests for methods in each layer. However, implementing end-to-end  tests can be quite labor-intensive. To achieve a return on the effort invested, I am d... + tags: showdev, go, testing, api\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 33. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  spectest, API Testing, Go, E2E, Markdown, Domain-Driven Design\n",
      "original keywords:  spectest, API Testing, Go, E2E Tests, Markdown, Domain-Driven Design\n",
      "text:  Data Cleansing & Manipulation - Data cleaning or Data cleansing and manipulation is a crucial step in a data project that involves identifying and correcting errors or…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 18. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Data Cleansing, Data Manipulation, Data Project\n",
      "original keywords:  Data Cleansing, Data Manipulation, Data Science\n",
      "text:  State Management with Nested Signals  - Experimental Angular state management with nested signals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 77. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  State Management, Angular, Nested Signals\n",
      "original keywords:  State Management, Angular, Nested Signals\n",
      "text:  Submit Black Friday Deal - Hey SaaS Founders, We're curating the ultimate list of top SaaS deals in our SaaS community, and we want your startup to be featured. Got an incredible Black Friday offer of your SaaS startup? Share it with us through the form below, we will include your SaaS offer to our list: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Black Friday, SaaS, Startup Offer\n",
      "original keywords:  Black Friday, SaaS, Startup, Community, Deals\n",
      "text:  Harnessing the Power of Behavioural Data in the Insurance Industry - Companies are progressively employing behavioral data analytics within the insurance sector to acquire more profound insights into customer…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 37. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Behavioural Data, Insurance Industry, Analytics\n",
      "original keywords:  Behavioural Data, Insurance Industry\n",
      "text:  Understanding Feature Importance in Machine Learning: Beyond the Numbers - In the realm of machine learning, the concept of feature importance plays a crucial role in understanding how models make predictions…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 102. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Feature Importance, Machine Learning, Predictions\n",
      "original keywords:  Feature Importance, Machine Learning\n",
      "text:  Python Kullanarak Kitap Öneri Sistemi Oluşturmanın Temel Adımları (TFIDF ve Cosine Similarty… - Özet:  Kitap okuma deneyimini kişiselleştirmek için Python kullanarak nasıl bir kitap öneri sistemi oluşturabileceğinizi adım adım…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 85. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Python, Temel Adımlar, TFIDF, Cosine Similarty\n",
      "original keywords:  Python, Recommendation System, Book Recommendation, TFIDF, Cosine Similarity\n",
      "text:  Advise for moving forward - It should be noted that I’m not a programmer. The extent of my programming has been manipulating calculation functions in already existing code. In other words, no experience. I work for a company that develops scanners and optimizers for lumber industry. With this comes the user interface to observe solutions based on data that the scanner sees and the parameter set in the optimizer. My company\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 27. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Programming, Optimizers, Lumber Industry, User Interface\n",
      "original keywords:  Programming, Lumber Industry, User Interface, Scanners, Optimizers\n",
      "text:  JavaScript Project with Zero JavaScript Knowledge?  - Can’t be that hard? Same as Java without the Script…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  JavaScript, Java, Project Management, Knowledge\n",
      "original keywords:  JavaScript, Java\n",
      "text:  Top 20 Front-End Interview Questions You Must Know - In this article, you’ll uncover a handpicked collection of some of the most prominent questions you might encounter in a frontend web…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Front-End Interview Questions, Web Development, Interview Questions\n",
      "original keywords:  Front-End, Interview Questions\n",
      "text:  Show HN: A custom GPT conversational language tutor that generates Anki cards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 12. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  GPT, Conversational Language Tutor, Anki Cards\n",
      "original keywords:  GPT, Anki, Conversational Language Tutor\n",
      "text:  DEM: Open-source containerized Development Environment Manager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 37. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  DEM, Containerized Development Environment Manager, Open Source\n",
      "original keywords:  DEM, Open-source, Containerization, Development Environment Manager\n",
      "text:  Exploring the Power and Simplicity of the Go Programming Language - Diving into Go’s Features, from Concurrency to Web Development, with Code Examples and Best Practices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 12. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Go, Programming Language, Concurrency, Web Development\n",
      "original keywords:  Go Programming Language, Concurrency, Web Development\n",
      "text:  Bytes: A fast, lightweight and customizable text editor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Bytes, Text Editor, Lightweight, Customizable\n",
      "original keywords:  Bytes, Text Editor\n",
      "text:  Why we're building Scanner: data lake search must be fastscanner.dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 25. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Scanner, Data Lake Search, Fast Search\n",
      "original keywords:  Scanner, Data Lake, Search Performance, Performance\n",
      "text:  In-depth Understanding to Optimize the Performance of Artificial Neural Network - Hyper-parameters tuning for deep learning techniques\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 35. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Artificial Neural Network, Deep Learning, Hyperparameters Tuning\n",
      "original keywords:  Artificial Neural Network, Hyper-parameters, Deep Learning, Performance Optimization\n",
      "text:  Deep Dive into Continuous Distributions and Bayesian Inference - In the realms of statistics and probability theory, continuous distributions are pivotal for modeling and understanding various types of…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 20. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Continuous Distributions, Bayesian Inference, Statistics, Probability Theory\n",
      "original keywords:  Continuous Distributions, Bayesian Inference, Statistics, Probability Theory\n",
      "text:  How To Setup A Cloudflare Tunnel And Expose Your Local Service Or Application. -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 16. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Cloudflare, Tunnel, Local Service, Application\n",
      "original keywords:  Cloudflare Tunnel, Cloudflare, Local Service, Application Exposure\n",
      "text:  Joby Performs First EVTOL Test Flights in New York\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Joby, EVTOL, Test Flights, New York\n",
      "original keywords:  Joby, EVTOL, Test Flights, New York\n",
      "text:  Find Your AI Solutions at the ODSC West AI Expo - At the AI Expo and Demo Hall as part of ODSC West in a few weeks, you’ll have the opportunity to meet one-on-one with representatives from…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  AI Solutions, ODSC West, AI Expo, Demo Hall\n",
      "original keywords:  AI Solutions, ODSC West, AI Expo\n",
      "text:  Python Fundamentals you should have a good understanding on as a beginner coder - From Vanilla JavaScript, React.js to Python and SQL in Just 6 Weeks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Python, JavaScript, React.js, SQL, Beginner Coding\n",
      "original keywords:  Python, JavaScript, React.js, SQL, Coding Fundamentals\n",
      "text:  How to write a program that shuts off pc after a few seconds of inactivity - I have never written a program, but I would like to make something which allows me to monitor either a program's activity or keyboard activity and shuts off my pc or a program after a set amount of time . The motivation is that I want to sometimes pressure myself into writing quickly, and something like this would force me to write my thoughts out on t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 43. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  , PC Shutoff, Keyboard Activity, Software Monitoring\n",
      "original keywords:  Program, PC Shutdown, Inactivity, Writing Productivity\n",
      "text:  The Foundation Models Reshaping Computer Vision - Learn about the Foundation Models — for object classification, object detection, and segmentation —  that are redefining Computer Vision.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 120. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Foundation Models, Computer Vision, Object Classification, Object Detection, Segmentation\n",
      "original keywords:  Foundation Models, Computer Vision, Object Classification, Object Detection, Segmentation\n",
      "text:  Unable to SSH into AWS hosted Ubuntu server. What am I missing? - I created a VPC, a subnet within it, attached an internet gateway, created a network interface, attached an elastic IP to the network interface and stood up the sever within the subnet in said VPC. The server is the only infrastructure inside of the VPC. I am still not able to connect to the server using SSH. Would I need to attach the elastic IP to the server to make this happen? Or add a load balancer to the VPC that targets traffic to the server?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 80. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  AWS, Ubuntu, SSH, Elastic IP, Load Balancer\n",
      "original keywords:  SSH, AWS, Ubuntu, VPC, Elastic IP, Network Interface\n",
      "text:  Stop using Lambda Layers  - Lambda layers are a special packaging mechanism provided by AWS Lambda to manage dependencies for zip-based Lambda functions. Layers themselves are nothing more than a sparkling zip file, but they have a few interesting properties which prove useful in some cases. Unfortunately Lambda layers are als... + tags: aws, lambda, webdev, programming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Lambda Layers, AWS, Lambda, Web Development, Programming\n",
      "original keywords:  AWS, Lambda, Lambda Layers, Layers, Dependencies, Packaging\n",
      "text:  Satiresoft: Why aren’t we doing AI? - This week at Satiresoft…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 16. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Satiresoft, AI, AI Development\n",
      "original keywords:  Satiresoft, AI\n",
      "text:  At the Intersection of LLMs and Kernels - Research Roundup -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 18. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  LLMs, Kernels, Research Roundup\n",
      "original keywords:  LLMs, Kernels, Research\n",
      "text:  Alan Wake 2 is an unexpected visual marvel even on older GPUsxfire.com\n",
      "generated keywords:  Alan Wake 2, GPUs, Visual Wonders\n",
      "original keywords:  Alan Wake 2, GPUs, Visual Marvel\n",
      "text:  I am worried with how AI is improving propaganda - If you’ve been following the war in the Middle East on Twitter, you’ve probably noticed how difficult it’s been to distinguish what is real and what isn’t. Without getting into the gruesome details, a few days ago there was an image going around of a deceased child and there was heavy discussion into whether it was ai generated or real. There are some ai identifying tools but they appear to be very unreliable at the moment, so we don’t have a concrete way to make this distinction. I don’t know what the final findings were for that particular image, but either way it exemplifies how AI can be used to make dangerous propaganda. Apparently Google is developing a hidden digital watermark to identify the images generated by their AI’s. Would you be in favor of some kind of legislation making this mandatory for AI image generators? Are there any ramifications you can think of?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 30. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  AI, Propaganda, Google, Digital Watermark, Legislation\n",
      "original keywords:  AI, Propaganda, Digital Watermark, Google, Legislation\n",
      "text:  Frank & FuelStat Launch - This month the Fuelet Wallet team has introduced two analytic tools for the Fuel Network users. Those are:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 26. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated keywords:  Frank, FuelStat, Fuelet Wallet, Fuel Network\n",
      "original keywords:  Frank, FuelStat, Fuelet Wallet, Fuel Network, Analytics Tools\n",
      "text:  Big Tech on Defensive as Copyright Threat Looms Over Generative AI. - Photo by Xu Haiwei on Unsplash\n",
      "generated keywords:  Big Tech, Copyright Threat, Generative AI, Xu Haiwei\n",
      "original keywords:  Big Tech, Copyright, Generative AI, Xu Haiwei\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 50):\n",
    "    text_test = dataset['test'][i]['text']\n",
    "    keywords = dataset['test'][i]['keywords']\n",
    "    print(\"text: \", text_test)\n",
    "    print(\"generated keywords: \", pipe(text_test)[0]['summary_text'])\n",
    "    print(\"original keywords: \", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]\n",
      "\u001b[A\n",
      "training_args.bin: 100%|██████████| 5.18k/5.18k [00:00<00:00, 18.3kB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model.safetensors: 100%|██████████| 1.63G/1.63G [01:55<00:00, 14.1MB/s]\n",
      "Upload 2 LFS files: 100%|██████████| 2/2 [01:56<00:00, 58.10s/it] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/wbcmthh42/bart_tech_keywords/commit/aaa476e530d52c5ef3c58dd4029c1cdf966f6d6c', commit_message='wbcmthh42/tech-keywords-extractor_plp', commit_description='', oid='aaa476e530d52c5ef3c58dd4029c1cdf966f6d6c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you would replace your own name here\n",
    "# you do not need to create a repository beforehand\n",
    "trainer.push_to_hub(\"wbcmthh42/tech-keywords-extractor_plp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plp-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
